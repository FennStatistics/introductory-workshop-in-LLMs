{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7102c40",
   "metadata": {},
   "source": [
    "# Different ways to call the Llama models online\n",
    "\n",
    "## Information\n",
    "\n",
    "Inference API of Hugging Face exposes models that have large community interest and are in active use: https://huggingface.co/docs/api-inference/supported-models\n",
    "\n",
    "\n",
    "**Precondition**: create an Access Token (https://huggingface.co/settings/tokens), set up a pro account to use the larger LLMs like Llama-3-70B (https://huggingface.co/pricing#pro) and accept the META LLAMA 3 COMMUNITY LICENSE AGREEMENT for the two different Llama models:\n",
    "\n",
    "* for `meta-llama/Llama-3.1-8B-Instruct`: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "* for `meta-llama/Meta-Llama-3-70B-Instruct`: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "\n",
    "Remark: Llama models are published under the **META LLAMA 3 COMMUNITY LICENSE AGREEMENT**. The Meta Llama 3 Community License grants users a non-exclusive, royalty-free license (you not need to pay ongoing fees) to use, modify, and distribute Llama 3 materials, with requirements for attribution and naming conventions when creating derivative works. Users with over 700 million monthly active users need a separate license, and Meta disclaims all warranties and limits liability for any use of the materials.\n",
    "\n",
    "\n",
    "**Possibilities to call models online using an API**:\n",
    "To call the `meta-llama/Llama-3.1-8B-Instruct` or `meta-llama/Meta-Llama-3-70B-Instruct` models from Hugging Face, you can use several different methods depending on your preferences and technical requirements. \n",
    "\n",
    "Here are the most common approaches:\n",
    "\n",
    "1. Using the InferenceClient from Hugging Face\n",
    "2. Using the openai API from OpenAI\n",
    "3. Using langchain_huggingface from langchain\n",
    "\n",
    "\n",
    "\n",
    "*** \n",
    "**Background information**\n",
    "\n",
    "* ...\n",
    "\n",
    "\n",
    "***\n",
    "**Coding sources**\n",
    "\n",
    "* You can run the `meta-llama/Llama-3.1-8B-Instruct`, see model page: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "* You can run the `meta-llama/Meta-Llama-3-70B-Instruct`, see model page: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    + Hugging Face documentation: https://huggingface.co/docs/transformers/main/en/model_doc/llama3\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "Exemplify different approaches to call Llama (LLMs) online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eb47a",
   "metadata": {},
   "source": [
    "## Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded within the single code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3711c0b",
   "metadata": {},
   "source": [
    "## Get API key(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f150b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d3cf",
   "metadata": {},
   "source": [
    "Create simple prompts, which is identical for all of the following approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03204fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n",
      "/tmp/ipykernel_119173/2372800891.py:3: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  user_content = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant specialized on animal names.\"\n",
    "user_content = \"\"\"\n",
    " Please write down five animals, provide only the names seperated by comma (\\,).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aca6f4",
   "metadata": {},
   "source": [
    "# Online approaches\n",
    "\n",
    "When using the online approaches we using the Serverless Inference API, see: https://huggingface.co/docs/api-inference/index\n",
    "\n",
    "## using the \"InferenceClient\" from huggingface_hub\n",
    "\n",
    "**Technical Considerations:**\n",
    "\n",
    "When querying the API, outputs are automatically cached if the inputs are identical. This applies to our case, as explained in the documentation: https://huggingface.co/docs/api-inference/parameters\n",
    "\n",
    "> To bypass caching and ensure fresh results for each query, it’s necessary to define the header: `\"x-use-cache\": \"false\"`, which is not possible using the OpenAI approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c959591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kangaroo, Lion, Penguin, Squirrel, Gorilla\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import textwrap\n",
    "client = InferenceClient(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)    # \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=500,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(text, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169d88",
   "metadata": {},
   "source": [
    "In general LLMs return **complex objects**, which contains \n",
    "\n",
    "it makes sense to store the complete objects and if needed add additional information:\n",
    "\n",
    "dir(output):\n",
    "* Purpose: Lists the names of the attributes and methods of the object.\n",
    "* Output Type: Returns a list of strings representing all attributes and methods.\n",
    "* Scope: Includes all members (both built-in and user-defined) of the object, such as methods and properties.\n",
    "\n",
    "vars(output)\n",
    "* Purpose: Returns the `__dict__` attribute of the object, which contains the object’s writable attributes.\n",
    "* Output Type: Returns a dictionary containing the object's writable attributes and their values.\n",
    "* Scope: Only includes the object's instance attributes; does not list methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef839cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get attributes and methods and writable attributes:\n",
      "\n",
      "Get attributes and methods:\n",
      " ['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'choices', 'clear', 'copy', 'created', 'fromkeys', 'get', 'id', 'items', 'keys', 'model', 'parse_obj', 'parse_obj_as_instance', 'parse_obj_as_list', 'pop', 'popitem', 'setdefault', 'system_fingerprint', 'update', 'usage', 'values']\n",
      "Get writable attributes:\n",
      " {'choices': [ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Kangaroo, Lion, Penguin, Squirrel, Gorilla', tool_calls=None), logprobs=None)], 'created': 1730200411, 'id': '', 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'system_fingerprint': '2.3.1-sha-a094729', 'usage': ChatCompletionOutputUsage(completion_tokens=14, prompt_tokens=62, total_tokens=76)}\n",
      "To get generated text:\n",
      " Kangaroo, Lion, Penguin, Squirrel, Gorilla\n",
      "\n",
      "See complete object:\n",
      "\n",
      "Complete object:\n",
      " ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Kangaroo, Lion, Penguin, Squirrel, Gorilla', tool_calls=None), logprobs=None)], created=1730200411, id='', model='meta-llama/Llama-3.1-8B-Instruct', system_fingerprint='2.3.1-sha-a094729', usage=ChatCompletionOutputUsage(completion_tokens=14, prompt_tokens=62, total_tokens=76))\n",
      "At best add current date and time:\n",
      " 2024-10-29 12:13:31.184921\n"
     ]
    }
   ],
   "source": [
    "print(\"Get attributes and methods and writable attributes:\\n\")\n",
    "\n",
    "# Get attributes and methods\n",
    "attributes = dir(output)\n",
    "print(\"Get attributes and methods:\\n\", attributes)\n",
    "\n",
    "# Get writable attributes\n",
    "writable_attributes = vars(output)\n",
    "print(\"Get writable attributes:\\n\", writable_attributes)\n",
    "print(\"To get generated text:\\n\", output.choices[0].message.content)\n",
    "\n",
    "\n",
    "print(\"\\nSee complete object:\\n\")\n",
    "print(\"Complete object:\\n\", output)\n",
    "\n",
    "\n",
    "import datetime\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.datetime.now()\n",
    "print(\"At best add current date and time:\\n\", current_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabc570",
   "metadata": {},
   "source": [
    "an example using `stream=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b810306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ele\n",
      "Elephant\n",
      "Elephant,\n",
      "Elephant, C\n",
      "Elephant, Cheet\n",
      "Elephant, Cheetah\n",
      "Elephant, Cheetah,\n",
      "Elephant, Cheetah, Ko\n",
      "Elephant, Cheetah, Koala\n",
      "Elephant, Cheetah, Koala,\n",
      "Elephant, Cheetah, Koala, Nar\n",
      "Elephant, Cheetah, Koala, Narwh\n",
      "Elephant, Cheetah, Koala, Narwhal\n",
      "Elephant, Cheetah, Koala, Narwhal,\n",
      "Elephant, Cheetah, Koala, Narwhal, M\n",
      "Elephant, Cheetah, Koala, Narwhal, Mongoose\n",
      "Elephant, Cheetah, Koala, Narwhal, Mongoose\n"
     ]
    }
   ],
   "source": [
    "client = InferenceClient(model=model_name, headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)    # \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "\n",
    "# iterate and print stream\n",
    "text = \"\" \n",
    "import json  # Import the json module  for error handling \n",
    "try:\n",
    "    for chunk in output:\n",
    "        text += chunk.choices[0].delta.content\n",
    "        print(text)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decode error: {e}\")\n",
    "    print(f\"Raw output: {chunk}\")  # Display raw output for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6101c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See complete object:\n",
      "\n",
      "Complete object:\n",
      " <generator object _stream_chat_completion_response at 0x7bc7c078d620>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSee complete object:\\n\")\n",
    "print(\"Complete object:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427989ee",
   "metadata": {},
   "source": [
    "## using \"openai\" from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79845c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 12:13:32,412 - INFO - HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "Tiger\n",
      "Tiger,\n",
      "Tiger, C\n",
      "Tiger, Cheet\n",
      "Tiger, Cheetah\n",
      "Tiger, Cheetah,\n",
      "Tiger, Cheetah, Gor\n",
      "Tiger, Cheetah, Gorilla\n",
      "Tiger, Cheetah, Gorilla,\n",
      "Tiger, Cheetah, Gorilla, Kang\n",
      "Tiger, Cheetah, Gorilla, Kangaroo\n",
      "Tiger, Cheetah, Gorilla, Kangaroo,\n",
      "Tiger, Cheetah, Gorilla, Kangaroo, Arm\n",
      "Tiger, Cheetah, Gorilla, Kangaroo, Armad\n",
      "Tiger, Cheetah, Gorilla, Kangaroo, Armadillo\n",
      "Tiger, Cheetah, Gorilla, Kangaroo, Armadillo\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize the client, pointing it to one of the available models\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "    api_key=key.hugging_api_key\n",
    ")\n",
    "output = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "# iterate and print stream\n",
    "text = \"\"\n",
    "    \n",
    "for chunk in output:\n",
    "    # print(chunk)\n",
    "    text = text + chunk.choices[0].delta.content\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb89155",
   "metadata": {},
   "source": [
    "### motivation to use \"openai\" from OpenAI - add functionalities of openAI\n",
    "\n",
    "see docs: https://platform.openai.com/docs/quickstart\n",
    "\n",
    "like including:\n",
    "* Error Handling: A try-except block is added to catch any exceptions that might occur during the API call, logging the error for debugging purposes.\n",
    "* Logging: A logging setup is included to log both the successful generation of responses and any errors that occur, which can be very helpful for monitoring and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9fb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 12:13:33,126 - INFO - HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "Lion\n",
      "Lion,\n",
      "Lion, Elephant\n",
      "Lion, Elephant,\n",
      "Lion, Elephant, Kang\n",
      "Lion, Elephant, Kangaroo\n",
      "Lion, Elephant, Kangaroo,\n",
      "Lion, Elephant, Kangaroo, Penguin\n",
      "Lion, Elephant, Kangaroo, Penguin,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 12:13:33,379 - INFO - Generated response: Lion, Elephant, Kangaroo, Penguin, Tiger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lion, Elephant, Kangaroo, Penguin, Tiger\n",
      "Lion, Elephant, Kangaroo, Penguin, Tiger\n"
     ]
    }
   ],
   "source": [
    "# Initialize the client, pointing it to one of the available models\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "    api_key=key.hugging_api_key,\n",
    ")\n",
    "\n",
    "def generate_chat_response(model_name, system_content, user_content, max_tokens=500, temperature=1, stream=True):\n",
    "    try:\n",
    "        output = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ],\n",
    "            stream=stream,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        # Initialize an empty string for the response text\n",
    "        text = \"\"\n",
    "\n",
    "        # Iterate and print the stream\n",
    "        for chunk in output:\n",
    "            text += chunk.choices[0].delta.content\n",
    "            print(text)\n",
    "        \n",
    "        # Optionally log the complete response\n",
    "        logging.info(\"Generated response: %s\", text)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred: %s\", str(e))\n",
    "\n",
    "# Example usage of the function\n",
    "generate_chat_response(\n",
    "    model_name=model_name, \n",
    "    system_content=system_content, \n",
    "    user_content=user_content,\n",
    "    max_tokens=500, \n",
    "    temperature=0.7  # Adjust temperature for creativity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32bf15",
   "metadata": {},
   "source": [
    "### motivation to use \"openai\" from OpenAI - add functionalities of openAI AND langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54332281",
   "metadata": {},
   "source": [
    "1. create a prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19061a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='\\n<Context>\\nYou are a knowledgeable assistant whose task is to provide two arrays: one for the names of the capitals and another for the primary languages spoken in the provided list of countries. \\nRespond solely with the information requested, formatted as specified.\\n</Context>\\n\\n<Data Structure>\\nThe provided countries are simply an array of country names.\\n</Data Structure>\\n\\n<Task>\\nWrite a JSON object containing two arrays: \"capital_names\" for the names of the requested capitals and \"languages\" for the primary languages spoken in those capitals. \\nEnsure that the information is structured as follows:\\n\\n{\\n  \"capitals\": [\\n   {\\n      \"name\": \"name1\",\\n      \"language\": \"language1\"\\n    },\\n    {\\n      \"name\": \"name2\",\\n      \"language\": \"language2\"\\n    },\\n    {\\n      \"name\": \"name3\",\\n      \"language\": \"language3\"\\n    }\\n  ]\\n}\\n\\nPlease respond with the entire JSON structure as a dictionary called \"capitals\", exactly as shown above, without any additional formatting or text.\\n</Task>\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='\\nWhat are the capitals of Germany, USA, France, China, Australia, and South Africa, along with the languages spoken there?\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Simplified system template\n",
    "system_template = \"\"\"\n",
    "<Context>\n",
    "You are a knowledgeable assistant whose task is to provide two arrays: one for the names of the capitals and another for the primary languages spoken in the provided list of countries. \n",
    "Respond solely with the information requested, formatted as specified.\n",
    "</Context>\n",
    "\n",
    "<Data Structure>\n",
    "The provided countries are simply an array of country names.\n",
    "</Data Structure>\n",
    "\n",
    "<Task>\n",
    "Write a JSON object containing two arrays: \"capital_names\" for the names of the requested capitals and \"languages\" for the primary languages spoken in those capitals. \n",
    "Ensure that the information is structured as follows:\n",
    "\n",
    "{{\n",
    "  \"capitals\": [\n",
    "   {{\n",
    "      \"name\": \"name1\",\n",
    "      \"language\": \"language1\"\n",
    "    }},\n",
    "    {{\n",
    "      \"name\": \"name2\",\n",
    "      \"language\": \"language2\"\n",
    "    }},\n",
    "    {{\n",
    "      \"name\": \"name3\",\n",
    "      \"language\": \"language3\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Please respond with the entire JSON structure as a dictionary called \"capitals\", exactly as shown above, without any additional formatting or text.\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Simplified user template\n",
    "user_template = \"\"\"\n",
    "What are the capitals of {userinput}, along with the languages spoken there?\n",
    "\"\"\"\n",
    "\n",
    "# Set up the ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "# Test the invoke with simplified templates\n",
    "result = prompt_template.invoke({\"userinput\": \"Germany, USA, France, China, Australia, and South Africa\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df3027",
   "metadata": {},
   "source": [
    "2. create a  JSON schema for structured output [not needed]:\n",
    "\n",
    "! not working for my code (don't know why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "931a047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema for structured output\n",
    "json_schema = {\n",
    "    \"title\": \"Outputs\",\n",
    "    \"description\": \"Structured response detailing the capitals and languages spoken in those capitals.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"capitals\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"capital_names\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the capital city.\"\n",
    "                    },\n",
    "                    \"language\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The primary language spoken in the capital city.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"capital_names\", \"language\"]\n",
    "            },\n",
    "            \"description\": \"An array of objects containing the capital cities and their respective languages.\"\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"capitals\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a997e",
   "metadata": {},
   "source": [
    "3. apply model\n",
    "\n",
    "! if your JSON schema would work uncomment `structured_llm, chain`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9c8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 12:13:37,348 - INFO - HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 424\n",
      "\tPrompt Tokens: 275\n",
      "\tCompletion Tokens: 149\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "Total Tokens: 424\n",
      "Prompt Tokens: 275\n",
      "Completion Tokens: 149\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "model = ChatOpenAI(model=model_name, openai_api_key=key.hugging_api_key, openai_api_base=\"https://api-inference.huggingface.co/v1/\", max_tokens=500, temperature=0.2)\n",
    "\n",
    "# Configure the model with structured output\n",
    "#structured_llm = model.with_structured_output(json_schema, include_raw=True)\n",
    "#chain = prompt_template | structured_llm\n",
    "\n",
    "chain = prompt_template | model\n",
    "\n",
    "# Execute the model and output response details\n",
    "with get_openai_callback() as cb:\n",
    "    response = chain.invoke(\n",
    "        {\"userinput\": \"Germany, USA, France, China, Australia, Angola, Egypt\"}\n",
    "    )\n",
    "    print(cb)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc6d6e",
   "metadata": {},
   "source": [
    "4. extract information in a readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e80753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"capitals\": [\n",
      "    {\n",
      "      \"name\": \"Berlin\",\n",
      "      \"language\": \"German\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Washington D.C.\",\n",
      "      \"language\": \"English\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Paris\",\n",
      "      \"language\": \"French\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Beijing\",\n",
      "      \"language\": \"Mandarin Chinese\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Canberra\",\n",
      "      \"language\": \"English\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Luanda\",\n",
      "      \"language\": \"Portuguese\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cairo\",\n",
      "      \"language\": \"Arabic\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "              name          language\n",
      "0           Berlin            German\n",
      "1  Washington D.C.           English\n",
      "2            Paris            French\n",
      "3          Beijing  Mandarin Chinese\n",
      "4         Canberra           English\n",
      "5           Luanda        Portuguese\n",
      "6            Cairo            Arabic\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "# Load the JSON data\n",
    "data = json.loads(response.content)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data['capitals'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417fa708",
   "metadata": {},
   "source": [
    "## using the \"langchain_huggingface\" and highlighting special tokens\n",
    "\n",
    "see docs: https://python.langchain.com/docs/integrations/platforms/huggingface/\n",
    "\n",
    "and blog post: https://huggingface.co/blog/langchain\n",
    "\n",
    "\n",
    "**Prompt format for Llama >= 3.1:**\n",
    "It is possible to define **special tokens** within prompts to trigger all kinds of behaviours. For more information, see:\n",
    "\n",
    "* Prompt formats of llama.com (Meta) webpage: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
    "* on their GitHub page: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a24e2",
   "metadata": {},
   "source": [
    "Example 1 - Get names of the capitals of different countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dadac18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berlin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    huggingfacehub_api_token=key.hugging_api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "llm.invoke(\"\"\"          \n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "    You are a helpful assistant, who only response with the name of the capital asked for.\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "    What is the capital of Germany?\n",
    "    \n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5e01a",
   "metadata": {},
   "source": [
    "Example 2 - Zero shot function calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa454986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"get_weather(city='San Francisco', metric='celsius'), get_weather(city='Seattle', metric='celsius')\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    huggingfacehub_api_token=key.hugging_api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "llm.invoke(\"\"\"          \n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
    "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
    "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
    "also point it out. You should only return the function call in tools call sections.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke.\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get weather info for places\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"city\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the city to get the weather for\"\n",
    "                },\n",
    "                \"metric\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The metric for weather. Options are: celsius, fahrenheit\",\n",
    "                    \"default\": \"celsius\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the weather in SF and Seattle in celsius?\n",
    "\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab53bb",
   "metadata": {},
   "source": [
    "Example 3 - Create Python Code to check the answer to a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a7daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is a simple function to check if a number is prime: ``` def is_prime(n):     if n <= 1:\n",
      "return False     for i in range(2, int(n**0.5) + 1):         if n % i == 0:             return False\n",
      "return True ``` Now, let's use this function to check if the number 7 is prime: ```\n",
      "print(is_prime(7))  # Output: True ``` As expected, the output is `True`, indicating that 7 is\n",
      "indeed a prime number.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    "    huggingfacehub_api_token=key.hugging_api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "output = llm.invoke(\"\"\"          \n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Write code to check if number is prime. Use it to verify if number 7 is prime\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\")\n",
    "\n",
    "print('\\n')\n",
    "print('\\n'.join(textwrap.wrap(output, 100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
