{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7102c40",
   "metadata": {},
   "source": [
    "# Different ways to call the Llama models offline\n",
    "\n",
    "**Information**\n",
    "\n",
    "To call the `meta-llama/Llama-3.1-8B-Instruct`, `meta-llama/Llama-3.2-3B-Instruct`  or `meta-llama/Llama-3.2-1B-Instruct`  models from Hugging Face offline, you can use several different methods depending on your preferences and technical requirements. \n",
    "\n",
    "I am only showing here one of the most common approaches:\n",
    "\n",
    "1. Using the Hugging Face Transformers Library\n",
    "\n",
    "\n",
    "Remark: Llama models are published under the **META LLAMA 3 COMMUNITY LICENSE AGREEMENT**. The Meta Llama 3 Community License grants users a non-exclusive, royalty-free license(you not need to pay ongoing fees) to use, modify, and distribute Llama 3 materials, with requirements for attribution and naming conventions when creating derivative works. Users with over 700 million monthly active users need a separate license, and Meta disclaims all warranties and limits liability for any use of the materials.\n",
    "\n",
    "\n",
    "*** \n",
    "**Background information**\n",
    "\n",
    "* See the single model pages in the subsection\n",
    "* You could also run the `meta-llama/Meta-Llama-3-70B-Instruct`, see model page: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    + Hugging Face documentation llama 3 models: https://huggingface.co/docs/transformers/main/en/model_doc/llama3\n",
    "\n",
    "\n",
    "for more details checkout the documentation of the transformers library: https://huggingface.co/docs/hub/transformers; whereby pipelines simply everything (you only need to specifiy the task, everything in the background is automatically specified): https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "Exemplify different approaches to call Llama (LLMs) offline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ea88a",
   "metadata": {},
   "source": [
    "# Check if you can use your GPU and need to fall back on your CPU\n",
    "\n",
    "for Non-Macs:\n",
    "\n",
    "> torch.cuda.is_available(): This function checks if a CUDA-compatible GPU is available on the system. CUDA, or Compute Unified Device Architecture, is NVIDIA’s parallel computing architecture. If torch.cuda.is_available() returns True, it means you have a CUDA-enabled GPU, and PyTorch can use it to accelerate computations.\n",
    "\n",
    "for Macs:\n",
    "\n",
    "> torch.backends.mps.is_available(): This function checks if the system supports Apple’s Metal Performance Shaders (MPS) backend, an alternative to CUDA on Apple hardware. MPS is designed for Apple’s GPUs, especially on Macs with M1/M2 chips. If torch.backends.mps.is_available() returns True, PyTorch can leverage Apple's MPS for faster computations on those GPUs.\n",
    "\n",
    "\n",
    "**Aim:** By checking these options, the code determines the best available hardware accelerator for running PyTorch computations, allowing the model to run faster on supported GPUs compared to CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ea157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): False\n",
      "torch.backends.mps.is_available() False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if you can use a Graphics Processing Unit (GPU) else use your Central Processing Unit (CPU):\n",
    "#> Compute Unified Device Architecture\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "#> Apple Metal Performance SHader\n",
    "print(\"torch.backends.mps.is_available()\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448c24c",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "## Define model\n",
    "\n",
    "It is highly recommend to **use a small LLM**, because using some approaches the LLMs is downloaded and stored on your computer (the model weights). It is highly likely that you do not have sufficient CPU and RAM and storage or your disk to run the larger (70B) model locally! See Llama 3.1 requirements: https://llamaimodel.com/requirements/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eb47a",
   "metadata": {},
   "source": [
    "## Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded within the single code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3711c0b",
   "metadata": {},
   "source": [
    "## Get API key(s)\n",
    "\n",
    "\n",
    "normally not needed to provide your huggingface key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f150b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d3cf",
   "metadata": {},
   "source": [
    "Create simple prompts, which is identical for all of the following approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03204fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n",
      "/tmp/ipykernel_115411/2372800891.py:3: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  user_content = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant specialized on animal names.\"\n",
    "user_content = \"\"\"\n",
    " Please write down five animals, provide only the names seperated by comma (\\,).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ea788",
   "metadata": {},
   "source": [
    "# Local approaches using the Hugging Face Transformers Library\n",
    "\n",
    "see documentation: https://huggingface.co/docs/transformers/index\n",
    "\n",
    "## for text generation / summarization\n",
    "\n",
    "\n",
    "### lama-3.1-8B-Instruct\n",
    "see: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "\n",
    "code adapted from: https://huggingface.co/blog/llama31\n",
    "\n",
    "> The following snippet shows how to use meta-llama/Meta-Llama-3.1-8B-Instruct. It requires about 16 GB of VRAM, which fits many consumer GPUs. The same snippet works for meta-llama/Meta-Llama-3.1-70B-Instruct, which, at 140GB of VRAM & meta-llama/Meta-Llama-3.1-405B-Instruct (requiring 810GB VRAM), makes it a very interesting model for production use cases. Memory consumption can be further reduced by loading in 8-bit or 4-bit mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806bbf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51a17149ba94c85ae0ec6a48ba24b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fenn/Documents/env_python/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/fenn/Documents/env_python/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! I be Captain Chat, the scurvy dog o' a chatbot, at yer service! Me and me trusty keyboard be here to swab the decks o' yer questions and provide ye with the treasure o' knowledge ye seek! So hoist the sails and set course fer a swashbucklin' good time, matey!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\" # Use CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e5380",
   "metadata": {},
   "source": [
    "Remark: IQLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning, see for details: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b8d5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible on your hardware\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  pipe = pipeline(\n",
    "      \"text-generation\",\n",
    "      model=model_id,\n",
    "        model_kwargs={\n",
    "          \"torch_dtype\": torch.bfloat16,\n",
    "          \"quantization_config\": {\"load_in_4bit\": True}\n",
    "      },\n",
    "      device=\"cpu\",  # Use CPU\n",
    "  )\n",
    "else: \n",
    "  print(\"not possible on your hardware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693530a9",
   "metadata": {},
   "source": [
    "### Llama-3.2-3B-Instruct\n",
    "see: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874d6c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8147539272c245edbe4a4bc936ccce10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yer lookin' fer a swashbucklin' pirate, eh? Alright then, matey! I be Captain Clueless, the scurviest pirate chatbot to ever sail the Seven Seas... er, I mean, the digital seas! Me and me trusty parrot sidekick, Polly, be here to help ye navigate the choppiest o' waters and find the hidden treasure o' knowledge! So hoist the sails and set course fer adventure, me hearty!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\" # Use CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4c547",
   "metadata": {},
   "source": [
    "### lama-3.2-1B-Instruct\n",
    "\n",
    "see: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f75a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me hearty! Yer lookin' fer a swashbucklin' chatbot, eh? Alright then, listen close and I'll tell ye about meself. Me name be Captain Chat, the greatest pirate of all time... or at least, I be tellin' meself I be. Me knowledge be vast, me wit be sharp, and me love fer words be stronger than me love fer a good bottle o' rum. I be here to help ye navigate the seven seas o' knowledge, answer yer questions, and maybe even share a few tales o' me own adventures. So hoist the sails and set course fer a treasure trove o' info, me hearty!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\" # Use CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95adf769",
   "metadata": {},
   "source": [
    "## for generating word embeddings\n",
    "\n",
    "\n",
    "see docs: https://huggingface.co/sentence-transformers\n",
    "\n",
    "\n",
    "however I call a relative weak model:\n",
    "\n",
    "![embedder model](pics/relative%20weak%20model.JPG)\n",
    "\n",
    "see leaderboard: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793866b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I feel great this morning</th>\n",
       "      <td>-0.026462</td>\n",
       "      <td>-0.044373</td>\n",
       "      <td>0.072443</td>\n",
       "      <td>0.034525</td>\n",
       "      <td>0.089534</td>\n",
       "      <td>-0.050451</td>\n",
       "      <td>0.018811</td>\n",
       "      <td>0.071296</td>\n",
       "      <td>-0.020522</td>\n",
       "      <td>-0.043637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005689</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.049055</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>-0.027642</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.065253</td>\n",
       "      <td>0.017496</td>\n",
       "      <td>-0.022810</td>\n",
       "      <td>-0.036687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I am feeling very good today</th>\n",
       "      <td>-0.043895</td>\n",
       "      <td>-0.020341</td>\n",
       "      <td>0.066563</td>\n",
       "      <td>-0.006310</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>-0.040420</td>\n",
       "      <td>0.079304</td>\n",
       "      <td>-0.009700</td>\n",
       "      <td>-0.042920</td>\n",
       "      <td>-0.025988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045309</td>\n",
       "      <td>0.049151</td>\n",
       "      <td>-0.049057</td>\n",
       "      <td>0.017821</td>\n",
       "      <td>-0.018061</td>\n",
       "      <td>-0.010441</td>\n",
       "      <td>0.043070</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>-0.006016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I am feeling terrible</th>\n",
       "      <td>0.017495</td>\n",
       "      <td>-0.057904</td>\n",
       "      <td>0.033315</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.051957</td>\n",
       "      <td>-0.048159</td>\n",
       "      <td>0.007659</td>\n",
       "      <td>0.119096</td>\n",
       "      <td>0.029929</td>\n",
       "      <td>-0.068960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038813</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>-0.074585</td>\n",
       "      <td>-0.018391</td>\n",
       "      <td>-0.026449</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.051495</td>\n",
       "      <td>-0.009829</td>\n",
       "      <td>0.030009</td>\n",
       "      <td>-0.064299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2         3    \\\n",
       "I feel great this morning    -0.026462 -0.044373  0.072443  0.034525   \n",
       "I am feeling very good today -0.043895 -0.020341  0.066563 -0.006310   \n",
       "I am feeling terrible         0.017495 -0.057904  0.033315  0.001710   \n",
       "\n",
       "                                   4         5         6         7    \\\n",
       "I feel great this morning     0.089534 -0.050451  0.018811  0.071296   \n",
       "I am feeling very good today  0.025980 -0.040420  0.079304 -0.009700   \n",
       "I am feeling terrible         0.051957 -0.048159  0.007659  0.119096   \n",
       "\n",
       "                                   8         9    ...       374       375  \\\n",
       "I feel great this morning    -0.020522 -0.043637  ... -0.005689 -0.000328   \n",
       "I am feeling very good today -0.042920 -0.025988  ... -0.045309  0.049151   \n",
       "I am feeling terrible         0.029929 -0.068960  ...  0.038813  0.003014   \n",
       "\n",
       "                                   376       377       378       379  \\\n",
       "I feel great this morning    -0.049055  0.016308 -0.027642  0.017276   \n",
       "I am feeling very good today -0.049057  0.017821 -0.018061 -0.010441   \n",
       "I am feeling terrible        -0.074585 -0.018391 -0.026449  0.005867   \n",
       "\n",
       "                                   380       381       382       383  \n",
       "I feel great this morning     0.065253  0.017496 -0.022810 -0.036687  \n",
       "I am feeling very good today  0.043070  0.018440 -0.008274 -0.006016  \n",
       "I am feeling terrible         0.051495 -0.009829  0.030009 -0.064299  \n",
       "\n",
       "[3 rows x 384 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define sentences\n",
    "sentences = [\n",
    "    \"I feel great this morning\",\n",
    "    \"I am feeling very good today\",\n",
    "    \"I am feeling terrible\"\n",
    "]\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract features\n",
    "features = model.encode(sentences)\n",
    "\n",
    "\n",
    "# Print the features as a pandas dataframe\n",
    "pd.DataFrame(features, index=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a2674d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7923, 0.5926],\n",
      "        [0.7923, 1.0000, 0.5782],\n",
      "        [0.5926, 0.5782, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "similarities = model.similarity(features, features)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7829294",
   "metadata": {},
   "source": [
    "# get information which models you have downloaded:\n",
    "\n",
    "see: https://huggingface.co/docs/huggingface_hub/guides/manage-cache\n",
    "\n",
    "in terminal:\n",
    "\n",
    "```\n",
    "huggingface-cli scan-cache\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
