{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7102c40",
   "metadata": {},
   "source": [
    "# Different ways to call the Llama models offline\n",
    "\n",
    "**Information**\n",
    "\n",
    "To call the `meta-llama/Llama-3.1-8B-Instruct`, `meta-llama/Llama-3.2-3B-Instruct`  or `meta-llama/Llama-3.2-1B-Instruct`  models from Hugging Face offline, you can use several different methods depending on your preferences and technical requirements. \n",
    "\n",
    "I am only showing here one of the most common approaches:\n",
    "\n",
    "1. Using the Hugging Face Transformers Library\n",
    "\n",
    "\n",
    "Remark: Llama models are published under the **META LLAMA 3 COMMUNITY LICENSE AGREEMENT**. The Meta Llama 3 Community License grants users a non-exclusive, royalty-free license(you not need to pay ongoing fees) to use, modify, and distribute Llama 3 materials, with requirements for attribution and naming conventions when creating derivative works. Users with over 700 million monthly active users need a separate license, and Meta disclaims all warranties and limits liability for any use of the materials.\n",
    "\n",
    "\n",
    "*** \n",
    "**Background information**\n",
    "\n",
    "* See the single model pages in the subsection\n",
    "* You could also run the `meta-llama/Meta-Llama-3-70B-Instruct`, see model page: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    + Hugging Face documentation llama 3 models: https://huggingface.co/docs/transformers/main/en/model_doc/llama3\n",
    "\n",
    "\n",
    "for more details checkout the documentation of the transformers library: https://huggingface.co/docs/hub/transformers; whereby pipelines simply everything (you only need to specifiy the task, everything in the background is automatically specified): https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "\n",
    "It is highly recommend to **use a small LLM in general**, because using some approaches the LLMs is downloaded and stored on your computer (the model weights). It is highly likely that you do not have sufficient CPU and RAM and storage or your disk to run the larger (70B) model locally! See Llama 3.1 requirements: https://llamaimodel.com/requirements/\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "Exemplify different approaches to call Llama (LLMs) offline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ea88a",
   "metadata": {},
   "source": [
    "# Check if you can use your GPU and need to fall back on your CPU\n",
    "\n",
    "for Non-Macs:\n",
    "\n",
    "> torch.cuda.is_available(): This function checks if a CUDA-compatible GPU is available on the system. CUDA, or Compute Unified Device Architecture, is NVIDIA’s parallel computing architecture. If torch.cuda.is_available() returns True, it means you have a CUDA-enabled GPU, and PyTorch can use it to accelerate computations.\n",
    "\n",
    "for Macs:\n",
    "\n",
    "> torch.backends.mps.is_available(): This function checks if the system supports Apple’s Metal Performance Shaders (MPS) backend, an alternative to CUDA on Apple hardware. MPS is designed for Apple’s GPUs, especially on Macs with M1/M2 chips. If torch.backends.mps.is_available() returns True, PyTorch can leverage Apple's MPS for faster computations on those GPUs.\n",
    "\n",
    "\n",
    "**Aim:** By checking these options, the code determines the best available hardware accelerator for running PyTorch computations, allowing the model to run faster on supported GPUs compared to CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# check if you can use a Graphics Processing Unit (GPU) else use your Central Processing Unit (CPU):\n",
    "#> Compute Unified Device Architecture\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "#> Apple Metal Performance SHader\n",
    "print(\"torch.backends.mps.is_available()\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448c24c",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eb47a",
   "metadata": {},
   "source": [
    "## Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded within the single code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3711c0b",
   "metadata": {},
   "source": [
    "## Get API key(s)\n",
    "\n",
    "\n",
    "normally not needed to provide your huggingface key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f150b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d3cf",
   "metadata": {},
   "source": [
    "Create simple prompts, which is identical for all of the following approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03204fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant specialized on animal names.\"\n",
    "user_content = \"\"\"\n",
    " Please write down five animals, provide only the names seperated by comma (\\,).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ea788",
   "metadata": {},
   "source": [
    "# Local approaches using the Hugging Face Transformers Library\n",
    "\n",
    "see documentation: https://huggingface.co/docs/transformers/index\n",
    "\n",
    "## for text generation / summarization\n",
    "\n",
    "\n",
    "### lama-3.1-8B-Instruct\n",
    "see: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "\n",
    "code adapted from: https://huggingface.co/blog/llama31\n",
    "\n",
    "> The following snippet shows how to use meta-llama/Meta-Llama-3.1-8B-Instruct. It requires about 16 GB of VRAM, which fits many consumer GPUs. The same snippet works for meta-llama/Meta-Llama-3.1-70B-Instruct, which, at 140GB of VRAM & meta-llama/Meta-Llama-3.1-405B-Instruct (requiring 810GB VRAM), makes it a very interesting model for production use cases. Memory consumption can be further reduced by loading in 8-bit or 4-bit mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\" # Use CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n",
    "\n",
    "\n",
    "# End the timer and print the runtime\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e5380",
   "metadata": {},
   "source": [
    "Remark: IQLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning, see for details: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  pipe = pipeline(\n",
    "      \"text-generation\",\n",
    "      model=model_id,\n",
    "        model_kwargs={\n",
    "          \"torch_dtype\": torch.bfloat16,\n",
    "          \"quantization_config\": {\"load_in_4bit\": True}\n",
    "      },\n",
    "      device=\"cpu\",  # Use CPU\n",
    "  )\n",
    "else: \n",
    "  print(\"not possible on your hardware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693530a9",
   "metadata": {},
   "source": [
    "### Llama-3.2-3B-Instruct\n",
    "see: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\" # Use CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n",
    "\n",
    "\n",
    "# End the timer and print the runtime\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cad39353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ebefe0add9492187778900ef903336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      "[{'generated_text': '\\nWrite a concise, well-structured scientific paragraph based strictly on the following guidelines:\\n\\nStructure:\\n1. **Topic Sentence (Beginning)** – Clearly introduce the main idea or core message of the paragraph concisely and scientifically. It must directly relate to the question and the context.\\n2. **Body (Middle)** – Develop the idea with logical reasoning, relevant evidence, explanations, and examples drawn from the provided context. Ensure a coherent flow of sentences.\\n3. **Conclusion (End)** – Summarize or reinforce the main point, aligning it with the topic sentence without introducing new ideas.\\n\\nAdditional instructions:\\n- **Only use the provided context** to generate the response. No external knowledge should be included.\\n- The paragraph must **directly answer the research question** using evidence and information from the context.\\n- Maintain a **formal, precise, and scientific** tone.\\n- The paragraph must focus on the research question and **follow a logical flow**.\\n\\n- **Do not** explicitly mention structural terms like \"topic sentence,\" \"body,\" or \"conclusion.\"\\n- **Do not** include introductory phrases such as \"Based on the provided context\" or \"The literature suggests.\" \\n\\n- **Return only the scientific paragraph** with no additional commentary.\\n- **Do not repeat the structural instructions** or the **additional instructions** in the response.\\n\\n---\\n\\n**Context:**  \\n\\nArtificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction. AI has applications in various fields, such as robotics, machine learning, and natural language processing.\\n\\n\\n**Research Question:**  \\nWhat is Artificial Intelligence?\\n\\n---\\n\\n**Scientific Paragraph Response:**  \\nArtificial intelligence is a computer system that simulates human intelligence processes, including learning, reasoning, and self-correction. This is achieved through complex algorithms and data structures that enable machines to analyze and process vast amounts of information. The development of AI has led to significant advancements in fields such as robotics, machine learning, and natural language processing. By mimicking human intelligence, AI systems can perform tasks that would be challenging or impossible for humans, including image recognition, speech recognition, and decision-making. Overall, AI has the potential to revolutionize numerous industries and aspects of our lives, transforming the way we interact with technology and each other.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Define the local model function\n",
    "def local_model_call(prompt_template, context, question, model_id=\"meta-llama/Llama-3.2-3B-Instruct\", max_tokens=256):\n",
    "    # Combine the prompt_template, context, and question\n",
    "    combined_prompt = prompt_template.format(context=context, question=question)\n",
    "    \n",
    "    # Initialize the model pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device=\"cpu\"  # Use CPU\n",
    "    )\n",
    "\n",
    "    # Call the model with the combined prompt\n",
    "    outputs = pipe(\n",
    "        combined_prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    # Get the generated response (outputs is a list of strings)\n",
    "    response = outputs # outputs[0]  # Directly take the string from the list\n",
    "    return response\n",
    "\n",
    "# Your custom PROMPT_TEMPLATE\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Write a concise, well-structured scientific paragraph based strictly on the following guidelines:\n",
    "\n",
    "Structure:\n",
    "1. **Topic Sentence (Beginning)** – Clearly introduce the main idea or core message of the paragraph concisely and scientifically. It must directly relate to the question and the context.\n",
    "2. **Body (Middle)** – Develop the idea with logical reasoning, relevant evidence, explanations, and examples drawn from the provided context. Ensure a coherent flow of sentences.\n",
    "3. **Conclusion (End)** – Summarize or reinforce the main point, aligning it with the topic sentence without introducing new ideas.\n",
    "\n",
    "Additional instructions:\n",
    "- **Only use the provided context** to generate the response. No external knowledge should be included.\n",
    "- The paragraph must **directly answer the research question** using evidence and information from the context.\n",
    "- Maintain a **formal, precise, and scientific** tone.\n",
    "- The paragraph must focus on the research question and **follow a logical flow**.\n",
    "\n",
    "- **Do not** explicitly mention structural terms like \"topic sentence,\" \"body,\" or \"conclusion.\"\n",
    "- **Do not** include introductory phrases such as \"Based on the provided context\" or \"The literature suggests.\" \n",
    "\n",
    "- **Return only the scientific paragraph** with no additional commentary.\n",
    "- **Do not repeat the structural instructions** or the **additional instructions** in the response.\n",
    "\n",
    "---\n",
    "\n",
    "**Context:**  \n",
    "{context}\n",
    "\n",
    "**Research Question:**  \n",
    "{question}\n",
    "\n",
    "---\n",
    "\n",
    "**Scientific Paragraph Response:**  \n",
    "\"\"\"\n",
    "\n",
    "# Example context and question\n",
    "context = \"\"\"\n",
    "Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction. AI has applications in various fields, such as robotics, machine learning, and natural language processing.\n",
    "\"\"\"\n",
    "question = \"What is Artificial Intelligence?\"\n",
    "\n",
    "# Call the local model function\n",
    "response = local_model_call(PROMPT_TEMPLATE, context, question)\n",
    "\n",
    "# Print the result\n",
    "print(\"Model Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8963a29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite a concise, well-structured scientific paragraph based strictly on the following guidelines:\\n\\nStructure:\\n1. **Topic Sentence (Beginning)** – Clearly introduce the main idea or core message of the paragraph concisely and scientifically. It must directly relate to the question and the context.\\n2. **Body (Middle)** – Develop the idea with logical reasoning, relevant evidence, explanations, and examples drawn from the provided context. Ensure a coherent flow of sentences.\\n3. **Conclusion (End)** – Summarize or reinforce the main point, aligning it with the topic sentence without introducing new ideas.\\n\\nAdditional instructions:\\n- **Only use the provided context** to generate the response. No external knowledge should be included.\\n- The paragraph must **directly answer the research question** using evidence and information from the context.\\n- Maintain a **formal, precise, and scientific** tone.\\n- The paragraph must focus on the research question and **follow a logical flow**.\\n\\n- **Do not** explicitly mention structural terms like \"topic sentence,\" \"body,\" or \"conclusion.\"\\n- **Do not** include introductory phrases such as \"Based on the provided context\" or \"The literature suggests.\" \\n\\n- **Return only the scientific paragraph** with no additional commentary.\\n- **Do not repeat the structural instructions** or the **additional instructions** in the response.\\n\\n---\\n\\n**Context:**  \\n\\nArtificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction. AI has applications in various fields, such as robotics, machine learning, and natural language processing.\\n\\n\\n**Research Question:**  \\nWhat is Artificial Intelligence?\\n\\n---\\n\\n**Scientific Paragraph Response:**  \\nArtificial intelligence is a computer system that simulates human intelligence processes, including learning, reasoning, and self-correction. This is achieved through complex algorithms and data structures that enable machines to analyze and process vast amounts of information. The development of AI has led to significant advancements in fields such as robotics, machine learning, and natural language processing. By mimicking human intelligence, AI systems can perform tasks that would be challenging or impossible for humans, including image recognition, speech recognition, and decision-making. Overall, AI has the potential to revolutionize numerous industries and aspects of our lives, transforming the way we interact with technology and each other.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4c547",
   "metadata": {},
   "source": [
    "### lama-3.2-1B-Instruct\n",
    "\n",
    "see: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\" # Use CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n",
    "\n",
    "\n",
    "# End the timer and print the runtime\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95adf769",
   "metadata": {},
   "source": [
    "## for generating word embeddings\n",
    "\n",
    "\n",
    "see docs: https://huggingface.co/sentence-transformers\n",
    "\n",
    "\n",
    "however I call a relative weak model:\n",
    "\n",
    "![embedder model](pics/relative%20weak%20model.JPG)\n",
    "\n",
    "see leaderboard: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793866b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define sentences\n",
    "sentences = [\n",
    "    \"I feel great this morning\",\n",
    "    \"I am feeling very good today\",\n",
    "    \"I am feeling terrible\"\n",
    "]\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract features\n",
    "features = model.encode(sentences)\n",
    "\n",
    "\n",
    "# Print the features as a pandas dataframe\n",
    "pd.DataFrame(features, index=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2674d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.similarity(features, features)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccca20",
   "metadata": {},
   "source": [
    "## for converting text to aduio (Text2Speech)\n",
    "\n",
    "see: https://huggingface.co/suno/bark-small\n",
    "\n",
    "\n",
    "* documentation on GitHub: https://github.com/suno-ai/bark\n",
    "* for different speakers, see: https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import time\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
    "model = AutoModel.from_pretrained(\"suno/bark-small\")\n",
    "\n",
    "voice_preset = \"v2/en_speaker_5\"\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"Hello, my name is Suno. And, uh — and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\"],\n",
    "    return_tensors=\"pt\",\n",
    "    voice_preset=voice_preset\n",
    ")\n",
    "\n",
    "speech_values = model.generate(**inputs, do_sample=True)\n",
    "\n",
    "# End the timer and print the runtime\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7635dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# play within Jupyter notebook:\n",
    "sampling_rate = model.generation_config.sample_rate\n",
    "Audio(speech_values.cpu().numpy().squeeze(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# Save as a WAV file\n",
    "sf.write(\"sample_suno.wav\", speech_values.cpu().numpy().squeeze(), sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbb742",
   "metadata": {},
   "source": [
    "## for converting audio to text (Speech2Text)\n",
    "\n",
    "see: https://huggingface.co/openai/whisper-large-v3-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    "    # generate_kwargs={\"task\": \"translate\", \"language\": \"german\"} # translate the audio file, else whisper predicts the language of the source audio automatically and the source audio language is the same as the target text language.\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "\n",
    "# Get the audio sample data\n",
    "audio_array = sample[\"array\"]\n",
    "sample_rate = sample[\"sampling_rate\"]\n",
    "# Save as a WAV file\n",
    "sf.write(\"sample_audio.wav\", audio_array, sample_rate)\n",
    "\n",
    "# Transcripe to text\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "\n",
    "\n",
    "# End the timer and print the runtime\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for classification tasks\n",
    "\n",
    "\n",
    "### small LLM (67M params) for fine-tuning\n",
    "\n",
    "see: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "and documentation: https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n",
    "> a small, fast, cheap and light Transformer model trained by distilling BERT base, can be used either for masked language modeling or next sentence prediction, but it's mostly **intended to be fine-tuned on a downstream task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ab4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e38f4",
   "metadata": {},
   "source": [
    "### small LLM (67M params) for fine-tuning\n",
    "\n",
    "\n",
    "see: https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "    + model predicts if a text has either positive (1) or negative (0) sentiment\n",
    "\n",
    "> this model (\"SiEBERT\", prefix for \"Sentiment in English\") is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "print(sentiment_analysis(\"I love this!\"))\n",
    "# do not trust blindly the model:\n",
    "print(sentiment_analysis(\"Sometimes I love this, sometimes I hate this!\"))\n",
    "print(sentiment_analysis(\"Sometimes I hate this, sometimes I love this!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7829294",
   "metadata": {},
   "source": [
    "# get information which models you have downloaded:\n",
    "\n",
    "see: https://huggingface.co/docs/huggingface_hub/guides/manage-cache\n",
    "\n",
    "in terminal:\n",
    "\n",
    "```\n",
    "\n",
    "huggingface-cli scan-cache\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
