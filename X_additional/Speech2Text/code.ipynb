{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7102c40",
   "metadata": {},
   "source": [
    "# Speech2Text\n",
    "\n",
    "**Information**\n",
    "\n",
    "The Speech2Text model was proposed in the article [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171)\n",
    "\n",
    "FAIRSEQ S2T is a comprehensive toolkit for speech-to-text tasks, offering end-to-end workflows that streamline data preprocessing, model training, and inference. It supports a variety of advanced models, including RNN, Transformer, and Conformer architectures, and integrates with machine translation and language models for multi-task learning. Designed for scalability and efficiency, FAIRSEQ S2T includes tools for tokenization, mixed precision training, multi-GPU support, and error analysis, making it ideal for large-scale experiments in speech recognition and translation.\n",
    "\n",
    "Remark: FAIRSEQ S2T was developed by a team at Meta's Fundamental AI Research (FAIR)\n",
    "\n",
    "***\n",
    "**Coding sources**\n",
    "\n",
    "* Hugging Face model page: https://huggingface.co/facebook/s2t-small-librispeech-asr\n",
    "* Hugging Face documentation: https://huggingface.co/docs/transformers/main/en/model_doc/speech_to_text\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "Mimic the Advanced Speech-to-Text API of Google by (a) transciping audio file and (b) improve the transcription by using two LLMs, see Google API: https://cloud.google.com/speech-to-text/?hl=en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eb47a",
   "metadata": {},
   "source": [
    "# Load your own audio file\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cbc1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "# pip install torchaudio sentencepiece\n",
    "\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54433ab8",
   "metadata": {},
   "source": [
    "Code to transcripe your audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b330799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large language models are incredibly powerful tools that can understand and generate human like text\n",
      "making them invaluable for tasks like writing translation and coating you will now\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Speech2Text model and processor from Hugging Face (model weights are downloaded automatically)\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "# Path to your local audio file (e.g., .wav file)\n",
    "audio_file = \"record.wav\" # !!! change your audio file path / name here\n",
    "\n",
    "# Load the audio file using librosa and resample to 16,000 Hz\n",
    "audio_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "# Process the audio data and prepare input features for the model\n",
    "inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "\n",
    "# Generate transcription using the model\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "# Decode the generated ids into a transcription text\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Output the transcription in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(transcription[0], 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3dcd01",
   "metadata": {},
   "source": [
    "# Improve the transcription of your audio file\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af4b6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70f4c0",
   "metadata": {},
   "source": [
    "Get API key(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa104e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('../..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8979d6e",
   "metadata": {},
   "source": [
    "Code to improve your transcriped audio file:\n",
    "\n",
    "@Julius: improve promting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeba956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'large language models are incredibly powerful tools that can understand and generate human-like\n",
      "text, making them invaluable for tasks like writing, translation, and coding. You will now'\n"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "client = InferenceClient(token=key.hugging_api_key)\n",
    "\n",
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant, whos task is to improve the transcription of an audio file. Focus here on possible spelling errors and missing words.\"\n",
    "user_content = f\"\"\"\n",
    "    Check the transcription of the following audio file. Only provide the improved transcription:\n",
    "    \n",
    "    {transcription}\n",
    "\"\"\"\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "   model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    max_tokens=500,\n",
    "    stream=False,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(text, 100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
