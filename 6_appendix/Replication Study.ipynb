{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b84bd",
   "metadata": {},
   "source": [
    "# Try to create determnistic texts by calling openAI module\n",
    "\n",
    "idea was based on the following code: https://github.com/TonySimonovsky/prompt_engineering_experiments/blob/main/experiments/DeterministicResultsOpenAI/Deterministic%20Results%20in%20OpenAI%20(report).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fcb05",
   "metadata": {},
   "source": [
    "## Get API key(s)\n",
    "\n",
    "\n",
    "normally not needed to provide your huggingface key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e367c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90ac6b",
   "metadata": {},
   "source": [
    "## Create deterministic data\n",
    "\n",
    "define number of runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7018a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28299daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# set up client\n",
    "client = OpenAI(\n",
    "    api_key=key.openAI_key,\n",
    ")\n",
    "\n",
    "# Function to call OpenAI's API using the updated format\n",
    "def get_response(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0,  # Set to 0 for deterministic results\n",
    "    seed=555, # beta feature, but it allows you to obtain consistent results for every input submitted to GPT.\n",
    "    max_tokens=50 # only 50 tokens\n",
    ")\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Function to call OpenAI's API using the updated format\n",
    "def get_response2(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    top_p=0,  # Set to 0 for deterministic results\n",
    "    seed=555, # beta feature, but it allows you to obtain consistent results for every input submitted to GPT.\n",
    "    max_tokens=50 # only 50 tokens\n",
    ")\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate variations of prompts\n",
    "prompt = \"Highlight application cases for Large Language Models.\"\n",
    "\n",
    "\n",
    "# Collect responses\n",
    "responses = []\n",
    "responses2 = []\n",
    "for i in range(num_runs):\n",
    "    # print(i)\n",
    "    response = get_response(prompt)\n",
    "    response2 = get_response2(prompt)\n",
    "    responses.append(response)\n",
    "    responses2.append(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e224075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert the list to a DataFrame\n",
    "responses_df = pd.DataFrame(responses)\n",
    "# Export to Excel\n",
    "responses_df.to_excel(\"deterministicResponses.xlsx\", index=False)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "responses_df2 = pd.DataFrame(responses2)\n",
    "# Export to Excel\n",
    "responses_df2.to_excel(\"deterministicResponses2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0704bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1. Natural language processing: Large language models can be used for tasks such as text generation, sentiment analysis, language translation, and speech recognition.\n",
      "\n",
      "2. Chatbots and virtual assistants: Large language models can be used to create more advanced and human-like\n"
     ]
    }
   ],
   "source": [
    "print(len(responses))\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7f337",
   "metadata": {},
   "source": [
    "## Compute Levenshtein distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa5a523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Distance Score Levenshtein for temperature 0 and set seed: 73.34252873563219\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein as lev\n",
    "import numpy as np\n",
    "\n",
    "# Create a distance matrix\n",
    "n = len(responses)\n",
    "distance_matrix = np.zeros((n, n))\n",
    "\n",
    "# Compute Levenshtein distances\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i != j:\n",
    "            distance_matrix[i][j] = lev.distance(responses[i], responses[j])\n",
    "\n",
    "\n",
    "# Display similarity matrix and calculate mean similarity score\n",
    "mean_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "print(\"Mean Distance Score Levenshtein for temperature 0 and set seed:\", mean_distance)\n",
    "# Print the distance matrix\n",
    "#print(\"Levenshtein Distance Matrix:\")\n",
    "#print(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d0e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Distance Score Levenshtein for top_p 0 and set seed: 109.32183908045977\n"
     ]
    }
   ],
   "source": [
    "# for responses2\n",
    "import Levenshtein as lev\n",
    "import numpy as np\n",
    "\n",
    "# Create a distance matrix\n",
    "n = len(responses2)\n",
    "distance_matrix = np.zeros((n, n))\n",
    "\n",
    "# Compute Levenshtein distances\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i != j:\n",
    "            distance_matrix[i][j] = lev.distance(responses2[i], responses2[j])\n",
    "\n",
    "\n",
    "# Display similarity matrix and calculate mean similarity score\n",
    "mean_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "print(\"Mean Distance Score Levenshtein for top_p 0 and set seed:\", mean_distance)\n",
    "# Print the distance matrix\n",
    "#print(\"Levenshtein Distance Matrix:\")\n",
    "#print(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1139a7",
   "metadata": {},
   "source": [
    "## Measure similarity using TF-IDF and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1073069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Similarity Score for temperature 0 and set seed:  0.6829032744918881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Measure similarity using TF-IDF and cosine similarity\n",
    "vectorizer = TfidfVectorizer().fit_transform(responses)\n",
    "vectors = vectorizer.toarray()\n",
    "cosine_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Create a structured DataFrame for the similarity matrix using response indices\n",
    "similarity_df = pd.DataFrame(cosine_matrix, index=[f'Response {i+1}' for i in range(len(responses))],\n",
    "                             columns=[f'Response {i+1}' for i in range(len(responses))])\n",
    "\n",
    "# Display similarity matrix and calculate mean similarity score\n",
    "mean_similarity = np.mean(cosine_matrix[np.triu_indices_from(cosine_matrix, k=1)])\n",
    "print(\"Mean Similarity Score for temperature 0 and set seed: \", mean_similarity)\n",
    "#print(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3237bd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Similarity Score for top_p 0 and set seed:: 0.5563508164354505\n"
     ]
    }
   ],
   "source": [
    "# for responses2\n",
    "# Measure similarity using TF-IDF and cosine similarity\n",
    "vectorizer = TfidfVectorizer().fit_transform(responses2)\n",
    "vectors = vectorizer.toarray()\n",
    "cosine_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Create a structured DataFrame for the similarity matrix using response indices\n",
    "similarity_df = pd.DataFrame(cosine_matrix, index=[f'Response {i+1}' for i in range(len(responses2))],\n",
    "                             columns=[f'Response {i+1}' for i in range(len(responses2))])\n",
    "\n",
    "# Display similarity matrix and calculate mean similarity score\n",
    "mean_similarity = np.mean(cosine_matrix[np.triu_indices_from(cosine_matrix, k=1)])\n",
    "print(\"Mean Similarity Score for top_p 0 and set seed::\", mean_similarity)\n",
    "#print(similarity_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
