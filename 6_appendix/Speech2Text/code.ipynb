{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7102c40",
   "metadata": {},
   "source": [
    "# Speech2Text\n",
    "\n",
    "**Information**\n",
    "\n",
    "The Speech2Text model of OpenAI was proposed in the article [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)\n",
    "\n",
    "The **Whisper large-v3-turbo model** is a streamlined, faster version of OpenAI's Whisper large-v3, reducing decoding layers from 32 to 4 to improve speed with minimal quality loss. Trained on over 5 million hours of labeled data, this model is robust in zero-shot speech recognition and translation across diverse datasets and domains.\n",
    "\n",
    "\n",
    "Remark: Model is based on [openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3), which was developed by OpenAI\n",
    "\n",
    "***\n",
    "**Coding sources**\n",
    "\n",
    "* Hugging Face model page: https://huggingface.co/openai/whisper-large-v3-turbo\n",
    "* Hugging Face documentation: https://huggingface.co/docs/transformers/main/en/model_doc/whisper\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "Mimic the Advanced Speech-to-Text API of Google by i. generating an audio file (Text2Speech), ii. transcribing the audio file (Speech2Text) and iii. improve the transcription by using two LLMs; see Google API: https://cloud.google.com/speech-to-text/?hl=en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fcce0",
   "metadata": {},
   "source": [
    "# Transcripe your audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6ab50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fenn/Documents/env_python/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So, let's brainstorm potential applications for large language models. With their ability to process and generate human-like text, there are tons of possibilities. What comes to mind first? Definitely customer support. LLMs can handle a large volume of basic inquiries like troubleshooting and FAQs 24-7. This would free up human agents to focus on more complex issues. Agreed. They'd also be great for content creation. Think of generating marketing copy, blogs, or even personalized emails. It could save so much time and maintain brand voice consistent. Right. And education too. LLM's could serve as tutors, explaining concepts in various ways until a student understands, interactive, and responsive learning. Another area is healthcare. They could assist in medical documentation or patient prescreening, which could speed up processes in busy clinics. Also research, you know, analyzing large data sets, summarizing reports, or even helping draft papers. Researchers would save hours. Exactly. This huge potential in every industry. Our focus should be on balancing productivity gains with ethical considerations. Agreed. We need to ensure transparency and control, especially with sensitive information. Let's start drafting specific use cases for each sector. Sounds like a plan.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import soundfile as sf\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    "    # generate_kwargs={\"task\": \"translate\", \"language\": \"german\"} # translate the audio file, else whisper predicts the language of the source audio automatically and the source audio language is the same as the target text language.\n",
    ")\n",
    "\n",
    "# Load audio data from your local \"dialog_suno.wav\" file\n",
    "audio_file = \"../Text2Speech/dialog_suno.wav\"\n",
    "audio_input, sample_rate = sf.read(audio_file)\n",
    "\n",
    "# Process and transcribe the audio data\n",
    "result = pipe({\"array\": audio_input, \"sampling_rate\": sample_rate})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3dcd01",
   "metadata": {},
   "source": [
    "# Improve the transcription of your audio file\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Get API key(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa104e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('../..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8979d6e",
   "metadata": {},
   "source": [
    "Code to improve your transcriped audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeba956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, let's brainstorm potential applications for large language models. With their ability to process\n",
      "and generate human-like text, there are tons of possibilities. What comes to mind first? Definitely\n",
      "customer support. LLMs can handle a large volume of basic inquiries, like troubleshooting and FAQs,\n",
      "24/7. This would free up human agents to focus on more complex issues. Agreed. They'd also be great\n",
      "for content creation. Think of generating marketing copy, blogs, or even personalized emails. It\n",
      "could save so much time and maintain a brand voice consistently. Right. And education too. LLMs\n",
      "could serve as tutors, explaining concepts in various ways until a student understands, providing\n",
      "interactive and responsive learning. Another area is healthcare. They could assist in medical\n",
      "documentation or patient prescreening, which could speed up processes in busy clinics. Also,\n",
      "research - you know, analyzing large data sets, summarizing reports, or even helping draft papers.\n",
      "Researchers would save hours. Exactly. This huge potential exists in every industry. Our focus\n",
      "should be on balancing productivity gains with ethical considerations. Agreed. We need to ensure\n",
      "transparency and control, especially with sensitive information. Let's start drafting specific use\n",
      "cases for each sector. Sounds like a plan.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import textwrap\n",
    "\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = InferenceClient(token=key.hugging_api_key)\n",
    "\n",
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant, whos task is to improve the transcription of an audio file. Focus here on possible spelling errors and missing words.\"\n",
    "user_content = f\"\"\"\n",
    "    Check the transcription of the following audio file. Only provide the improved transcription:\n",
    "    \n",
    "    {transcription}\n",
    "\"\"\"\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "   model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    max_tokens=500,\n",
    "    stream=False,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(text, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482cc4e",
   "metadata": {},
   "source": [
    "# Summarize the transcribed audio file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c05402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1:\n",
      "- Large language models (LLMs) can be applied to various industries due to their ability to process and generate human-like text.\n",
      "- Potential applications include customer support, handling basic inquiries and freeing human agents for complex issues.\n",
      "- LLMs can aid in content creation, such as generating marketing copy, blogs, and personalized emails, saving time and maintaining a consistent brand voice.\n",
      "- In education, LLMs can serve as interactive tutors, explaining concepts in multiple ways for better student understanding.\n",
      "- Healthcare applications include assisting in medical documentation and patient prescreening, speeding up processes in busy clinics.\n",
      "- LLMs can also aid researchers in analyzing large data sets, summarizing reports, and drafting papers, saving time.\n",
      "- Balancing productivity gains with ethical considerations, ensuring transparency and control, especially with sensitive information, is crucial.\n",
      "\n",
      "List 2:\n",
      "- Draft specific use cases for each sector (customer support, content creation, education, healthcare, research) to explore the potential applications of large language models.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = InferenceClient(token=key.hugging_api_key)\n",
    "\n",
    "# Create prompts\n",
    "system_content = f\"\"\"\n",
    "<Context>\n",
    "You are a helpful assistant, whos task is to summarize transcribed audio files. \n",
    "</Context>\n",
    "\n",
    "<Task>\n",
    "Please provide two lists:.\n",
    "- Summarize the content regarding the most important points of discussion, be scientific\n",
    "- Provide a list of open tasks, which need to be done.\n",
    "\n",
    "Provide the second list only if there are any important tasks mentioned.\n",
    "</Task>\n",
    "\n",
    "\n",
    "<Data Structure>\n",
    "Provide the lists in the following format:\n",
    "\n",
    "List 1:\n",
    "\\\\n\n",
    "\\\\n\n",
    "List 2:\n",
    "</Data Structure>\n",
    "\"\"\"\n",
    "user_content = f\"\"\"\n",
    "    Provide the two lists regarding the following text:\n",
    "    {text}\n",
    "\"\"\"\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "   model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    max_tokens=500,\n",
    "    stream=False,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
