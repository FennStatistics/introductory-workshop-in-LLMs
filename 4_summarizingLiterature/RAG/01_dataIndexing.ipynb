{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745eb47a",
   "metadata": {},
   "source": [
    "# Data Indexing\n",
    "\n",
    "There are two central steps involved:\n",
    "\n",
    "1. Documents are stored, prepared and split into smaller text chunks.\n",
    "2. Text chunks are converted into vector embeddings and stored in a vector database (Vector DB) next to their respective text chunks.\n",
    "\n",
    "\n",
    "*** \n",
    "**Background information**\n",
    "\n",
    "* All files, chunks and embeddings are stored on a local **Supabase** server (open source Firebase alternative; based on Postgres, which is a relational database management system), see: https://supabase.com/\n",
    "\n",
    "\n",
    "***\n",
    "**Coding sources**\n",
    "\n",
    "I extend the code provided and explained in the following YouTube Video: \n",
    "\n",
    "- RAG Langchain Python Project: Easy AI/Chat For Your Docs: https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "    + GitHub: https://github.com/pixegami/langchain-rag-tutorial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## If you facing issues running your Code:\n",
    "\n",
    "It could be the case that chroma and langchain cause import issues, see: https://github.com/langchain-ai/langchain/issues/7509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b062d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run in your terminal:\n",
    "# pip install pydantic==1.10.8\n",
    "# pip install chromadb==0.3.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37501de6",
   "metadata": {},
   "source": [
    "## Get API, local supabase server key(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f150b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('../..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f770440",
   "metadata": {},
   "source": [
    "## include self-written functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4eadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.forDataIndexing as di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078994c",
   "metadata": {},
   "source": [
    "# Connect to our local supabase server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "\n",
    "supabase = create_client(key.SUPABASE_URL, key.SUPABASE_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce202d6",
   "metadata": {},
   "source": [
    "# Data Preperation: Documents are stored, prepared and split into smaller text chunks\n",
    "\n",
    "## Enter your search / how you have found the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = supabase.table(\"searches\").select(\"topic\").execute()\n",
    "# Convert the list of tuples to a dictionary\n",
    "data_dict = dict(res)\n",
    "# Access the 'data' key directly\n",
    "data_items = data_dict.get('data', [])\n",
    "# Extract topics from the list of dictionaries within 'data'\n",
    "topics = [entry['topic'] for entry in data_items]\n",
    "print(\"search topics in your DB:\\n\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_search = {'topic': \"AI regulation\", 'subtopic': \"trust, risk, benefit\", \n",
    "                'search_query':'\"artificial intelligence\" AND (trust OR risk* OR benef*) AND \"regulation\"', 'search_plattform': \"Google Scholar\", \n",
    "                'comment':\"only retrieved the first 30 entries (2 excluded, because 1x not downloadable); N=29\"}\n",
    "\n",
    "# Check if the 'topic' in entry_search is in topics\n",
    "if entry_search['topic'] in topics:\n",
    "    # Update the existing entry where the topic matches\n",
    "    data = supabase.table('searches').update(entry_search).eq('topic', entry_search['topic']).execute()\n",
    "    print(\"search entry updated\")\n",
    "else:\n",
    "    # Insert the new entry if the topic is not found in topics\n",
    "    data = supabase.table('searches').insert(entry_search).execute()\n",
    "    print(\"new search entry added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505788c0",
   "metadata": {},
   "source": [
    "## Upload your PDFs in the local DB\n",
    "\n",
    "define folder path to your PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16958e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_PDF = os.path.join('PDFs/AIregulation/')  # Moves one level up to 'PDFs' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075d9c1",
   "metadata": {},
   "source": [
    "upload PDFs to storage **AND** creates an entry in the DB:\n",
    "\n",
    "Remark: this function takes a bit, because PDFs are temporarly loaded to get their number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_Search = {'topic': \"AI regulation\", 'subtopic': \"trust, risk, benefit\"}\n",
    "\n",
    "di.upload_PDFs(folder_path=path_to_PDF, supabase_DB=supabase, args_Search=args_Search, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d1195",
   "metadata": {},
   "source": [
    "PDFs in your DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e45071",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = supabase.storage.from_('files').list()\n",
    "# Extracting 'name' from each dictionary\n",
    "file_names = [file['name'] for file in res]\n",
    "\n",
    "# Output the list of file names\n",
    "print(file_names)\n",
    "\n",
    "# Number of PDFs in DB\n",
    "print(len(file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c3d26",
   "metadata": {},
   "source": [
    "## PDFs in the local DB are prepared\n",
    "\n",
    "set a folder path for the temporary download of your PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932f931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_PDFs = os.path.join('tmp_downloads')  # Moves one level up to 'tmp_downloads' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2319c99",
   "metadata": {},
   "source": [
    "aaaaaaaaaaaaaaaaaa GROBID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93f6a0",
   "metadata": {},
   "source": [
    "https://grobid.readthedocs.io/en/latest/Grobid-docker/#grobid-and-docker-containers\n",
    "\n",
    "docker run --rm --gpus all --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81e28f",
   "metadata": {},
   "source": [
    "see: https://grobid.readthedocs.io/en/latest/Grobid-service/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee7960a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_file = Path(path_to_PDFs + \"/10.1007_s00146-023-01650-z.pdf\")\n",
    "files = {\n",
    "    'input': open(pdf_file, 'rb'),\n",
    "}\n",
    "\n",
    "response = requests.post('http://localhost:8070/api/processFulltextDocument', files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d033110",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d06a5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client import Client\n",
    "\n",
    "client = Client(base_url=\"https://cloud.science-miner.com/grobid/api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4958dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515dabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client.grobid_client import GrobidClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d789b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client import Client\n",
    "\n",
    "client = Client(base_url=\"http://localhost:8070/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5ac0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from grobid_client.api.pdf import process_fulltext_document\n",
    "from grobid_client.models import Article, ProcessForm\n",
    "from grobid_client.types import TEI, File\n",
    "pdf_file = \"MyPDFFile.pdf\"\n",
    "pdf_file = Path(path_to_PDFs + \"/10.1007_s00146-023-01650-z.pdf\")\n",
    "\n",
    "with pdf_file.open(\"rb\") as fin:\n",
    "    form = ProcessForm(\n",
    "        segment_sentences=\"1\",\n",
    "        input_=File(file_name=pdf_file.name, payload=fin, mime_type=\"application/pdf\"),\n",
    "    )\n",
    "    r = process_fulltext_document.sync_detailed(client=client, multipart_data=form)\n",
    "    if r.is_success:\n",
    "        article: Article = TEI.parse(r.content, figures=False)\n",
    "        assert article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c13a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580b5c0",
   "metadata": {},
   "source": [
    "aaaaaaaaaaaaaaaaaa GROBID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4990f2",
   "metadata": {},
   "source": [
    "get the names of all PDFs, which have not been processed (chunks + embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da37230",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_processed_PDFs = di.non_processed_PDFs(supabase_DB=supabase, verbose=False)\n",
    "print(\"non_processed_PDFs:\\n\", non_processed_PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf53a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_Split = {'chunk_size': 800, 'chunk_overlap': 150}\n",
    "\n",
    "di.load_split_embed(supabase_DB=supabase, path_to_PDFs=path_to_PDFs, args_Split=args_Split, LMM='all-MiniLM-L6-v2')\n",
    "# delete all downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    supabase.table(\"documents_chunks\")\n",
    "    .select(\"*\")\n",
    "    .eq(\"order_chunks\", 0)\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "print(len(response.data))\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f01f1a",
   "metadata": {},
   "source": [
    "# Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = supabase.rpc('hello_world3').execute()\n",
    "print(\"Hello World:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9113120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Define the namedtuple\n",
    "Document = namedtuple('Document', ['page_content'])\n",
    "\n",
    "# Create an instance of Document\n",
    "doc = Document(\"How can AI Regulation be defined?\")\n",
    "\n",
    "# Pass the object inside a list\n",
    "embedding = di.create_embeddings([doc], LMM='all-MiniLM-L6-v2', verbose=False)\n",
    "print(\"embedding:\\n\", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a74c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_threshold = 0.7  # Replace with your desired threshold\n",
    "\n",
    "# Call the RPC function\n",
    "data = supabase.rpc('match_documents_chunks', {\n",
    "    'embedding': embedding[0].tolist(),\n",
    "    'match_threshold': match_threshold,\n",
    "    'match_count': 22\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2763710",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbd003",
   "metadata": {},
   "source": [
    "**load_pdfs_by_filename**: Loads and stores PDF pages by filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pdf_pages = di.load_pdfs_by_filename(path_to_PDFs, verbose=False)\n",
    "\n",
    "# Optional: Print the loaded pages by filename\n",
    "for filename, pages in pdf_pages.items():\n",
    "    print(f\"\\nPDF: {filename}\")\n",
    "    print(f\"Total Pages: {len(pages)}\")\n",
    "    # print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8630b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pdf_chunks is the dictionary containing chunks for each PDF\n",
    "first_key = list(pdf_pages.keys())[0]  # Get the first PDF filename\n",
    "print(\"first PDF of folder:\", first_key)\n",
    "first_pdf_pages = pdf_pages[first_key]  # Get the chunks for the first PDF\n",
    "\n",
    "\n",
    "# Print the first page\n",
    "print(\"First Page:\", first_pdf_pages[0], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d581a",
   "metadata": {},
   "source": [
    "**split_pdf_pages_into_chunks**: Splits and stores PDF pages into chunks by filename:\n",
    "\n",
    "On average, one token is roughly 4 characters in English text. So, each chunk of 800 characters roughly corresponds to 200 tokens.\n",
    "\n",
    "\n",
    "**The maximal number of tokens of `all-MiniLM-L6-v2` is 512.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks = di.split_pdf_pages_into_chunks(pdf_pages, chunk_size=800, chunk_overlap=150, verbose=False)\n",
    "\n",
    "# Optional: Print a summary of chunks created per PDF\n",
    "for filename, chunks in pdf_chunks.items():\n",
    "    print(f\"\\nPDF: {filename}\")\n",
    "    print(f\"Total Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe313b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pdf_chunks is the dictionary containing chunks for each PDF\n",
    "first_key = list(pdf_chunks.keys())[0]  # Get the first PDF filename\n",
    "print(\"first PDF of folder:\", first_key)\n",
    "first_pdf_chunks = pdf_chunks[first_key]  # Get the chunks for the first PDF\n",
    "\n",
    "# Access the first and second chunks\n",
    "first_chunk = first_pdf_chunks[0]\n",
    "second_chunk = first_pdf_chunks[1]\n",
    "\n",
    "# Print the first two chunks\n",
    "print(\"\\nFirst Chunk:\", first_chunk, \"\\n\\n\")\n",
    "print(\"Second Chunk:\", second_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"page content:\", first_chunk.page_content, \"\\n\\n\")\n",
    "print(\"metadata:\", first_chunk.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e336786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second_chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_pdf_chunks)\n",
    "first_pdf_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pdf_chunks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4017f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assuming first_pdf_chunks is an array of PDF page objects or text chunks\n",
    "tmp_chunks = []  # Initialize an empty array to store the extracted content\n",
    "\n",
    "# Iterate over each page or chunk in first_pdf_chunks\n",
    "for chunk in first_pdf_chunks:\n",
    "    # Extract the page content (assuming 'chunk' has a method or property called 'extract_content')\n",
    "    content = chunk.page_content  # Modify this line based on how you extract content from your PDF chunks\n",
    "    # Append the extracted content to tmp_chunks\n",
    "    tmp_chunks.append(content)\n",
    "\n",
    "# tmp_chunks now contains the extracted content from each page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4ca22",
   "metadata": {},
   "source": [
    "# Data Storage: Text chunks are converted into vector embeddings and stored in a vector database (Vector DB) next to their respective text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc889403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define sentences\n",
    "sentences = tmp_chunks\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract features\n",
    "features = model.encode(sentences)\n",
    "\n",
    "# Print the features as a pandas dataframe\n",
    "pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.similarity(features, features)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model and encode the corpus\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "corpus = sentences\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "\n",
    "# Calculate similarity matrix using cosine similarity\n",
    "similarity_matrix = cosine_similarity(corpus_embeddings)\n",
    "\n",
    "# Convert similarity matrix to a distance matrix\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(distance_matrix, 'ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 8))\n",
    "dendrogram(linked, labels=corpus, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title(\"Dendrogram of Sentence Clustering\")\n",
    "plt.xlabel(\"Sentences\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e22fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "#from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "from faker import Faker\n",
    "import faker_commerce\n",
    "\n",
    "\n",
    "def add_entries_to_vendor_table(supabase, vendor_count):\n",
    "    fake = Faker()\n",
    "    foreign_key_list = []\n",
    "    fake.add_provider(faker_commerce.Provider)\n",
    "    main_list = []\n",
    "    for i in range(vendor_count):\n",
    "        value = {'vendor_name': fake.company(), 'total_employees': fake.random_int(40, 169),\n",
    "                 'vendor_location': fake.country()}\n",
    "\n",
    "        main_list.append(value)\n",
    "    data = supabase.table('vendor2').insert(main_list).execute()\n",
    "    data_json = json.loads(data.json())\n",
    "    data_entries = data_json['data']\n",
    "    for i in range(len(data_entries)):\n",
    "        foreign_key_list.append(int(data_entries[i]['vendor_id']))\n",
    "    return foreign_key_list\n",
    "\n",
    "\n",
    "def add_entries_to_product_table(supabase, vendor_id):\n",
    "    fake = Faker()\n",
    "    fake.add_provider(faker_commerce.Provider)\n",
    "    main_list = []\n",
    "    iterator = fake.random_int(1, 15)\n",
    "    for i in range(iterator):\n",
    "        value = {'vendor_id': vendor_id, 'product_name': fake.ecommerce_name(),\n",
    "                 'inventory_count': fake.random_int(1, 100), 'price': fake.random_int(45, 100)}\n",
    "        main_list.append(value)\n",
    "    data = supabase.table('Product').insert(main_list).execute()\n",
    "\n",
    "\n",
    "def main():\n",
    "    vendor_count = 10\n",
    "    supabase: Client = create_client(key.SUPABASE_URL, key.SUPABASE_KEY)\n",
    "    fk_list = add_entries_to_vendor_table(supabase, vendor_count)\n",
    "    #for i in range(len(fk_list)):\n",
    "    #    add_entries_to_product_table(supabase, fk_list[i])\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client\n",
    "\n",
    "supabase: Client = create_client(key.SUPABASE_URL, key.SUPABASE_KEY)\n",
    "\n",
    "data = supabase.rpc('hello_world').execute()\n",
    "print(\"Hello World:\", data)\n",
    "\n",
    "\n",
    "data = supabase.rpc('get_vendors').gt('total_employees', 160).execute()\n",
    "print(\"Vendors:\", data)\n",
    "vars(data)\n",
    "data.data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
