# Map word frequencies to vertex sizes
vertex_sizes <- sapply(vertices, function(word) {
freq <- word_frequencies %>%
filter(response == word) %>%
pull(frequency)
if(length(freq) == 0) {
return(1)  # Default size if the word is not found (shouldn't happen)
} else {
return(freq)
}
})
# Plot the network with vertex sizes proportional to word frequencies and edge widths proportional to edge weights
plot(g,
vertex.label.color = "black",
vertex.size = vertex_sizes * 0.5,           # Scale vertex sizes by word frequency
edge.width = edge_widths * 1,             # Scale edge widths by weight (number of links)
edge.arrow.size = 0.5)
# Return the igraph object
return(g)
}
tmp_g <- generate_association_network("underweight", associationData, distance = 2)
# interactive network:
E(tmp_g)$width <- 1
E(tmp_g)$weight <- E(tmp_g)$weight  / max(E(tmp_g)$weight )
E(tmp_g)[ weight > 0.3 ]$width <- 2
E(tmp_g)[ weight > 0.4 ]$width <- 3
E(tmp_g)[ weight > 0.5 ]$width <- 4
E(tmp_g)[ weight > 0.6 ]$width <- 5
E(tmp_g)[ weight > 0.7 ]$width <- 6
visIgraph(tmp_g)
## global variables:
# only if necessary to limit running time
#| echo: true
#| warning: false
# sets the directory of location of this script as the current directory
# setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# load packages
require(pacman)
p_load('DT', 'associatoR', 'tidyverse', 'wordcloud', 'readxl', 'igraph', 'visIgraph')
# load data
setwd("../generated data")
dir()
# for first order associations example
associationData <- read_excel(path = "word_associations_firstSecondOrder.xlsx", sheet = 1)
dim(associationData)
length(unique(associationData$participant_id))
table(associationData$participant_id)
sort(table(associationData$response), decreasing = TRUE)[1:10]
# Function to create edges from the grouped responses
create_edges <- function(association, distance = 2) {
edges <- data.frame()
n <- length(association)
for (i in 1:n) {
for (d in 1:distance) {
if (i + d <= n) {
edges <- rbind(edges, data.frame(from = association[i], to = association[i + d]))
}
}
}
return(edges)
}
generate_association_network <- function(cue_word, data_set, distance = 2) {
# Filter the dataset based on the cue word
tmp <- data_set[data_set$cue == cue_word, ]
# print number of times a response was given:
print("number of times a response was given")
print(sort(table(tmp$response)))
grouped_responses <- tmp %>%
group_by(participant_id) %>%
summarise(associations = list(response)) %>%
ungroup()
# Apply the function to create edges for each participant
all_edges <- do.call(rbind, lapply(grouped_responses$associations, create_edges, distance = distance))
# Calculate word frequencies across the entire data set
word_frequencies <- tmp %>%
count(response, name = "frequency")
# Calculate the edge weights (how often each link appears between two words)
edge_weights <- all_edges %>%
group_by(from, to) %>%
summarise(weight = n(), .groups = "drop")
# Create the graph from the edge list with edge weights
g <- graph_from_data_frame(edge_weights, directed = FALSE)
# Get the list of unique words in the graph (vertices)
vertices <- V(g)$name
# Extract edge weights to adjust the edge widths
edge_widths <- E(g)$weight
# Map word frequencies to vertex sizes
vertex_sizes <- sapply(vertices, function(word) {
freq <- word_frequencies %>%
filter(response == word) %>%
pull(frequency)
if(length(freq) == 0) {
return(1)  # Default size if the word is not found (shouldn't happen)
} else {
return(freq)
}
})
# Plot the network with vertex sizes proportional to word frequencies and edge widths proportional to edge weights
plot(g,
vertex.label.color = "black",
vertex.size = vertex_sizes * 0.5,           # Scale vertex sizes by word frequency
edge.width = edge_widths * 1,             # Scale edge widths by weight (number of links)
edge.arrow.size = 0.5)
# Return the igraph object
return(g)
}
tmp_g <- generate_association_network("underweight", associationData, distance = 2)
# interactive network:
E(tmp_g)$width <- 1
E(tmp_g)$weight <- E(tmp_g)$weight  / max(E(tmp_g)$weight )
E(tmp_g)[ weight > 0.3 ]$width <- 2
E(tmp_g)[ weight > 0.4 ]$width <- 3
E(tmp_g)[ weight > 0.5 ]$width <- 4
E(tmp_g)[ weight > 0.6 ]$width <- 5
E(tmp_g)[ weight > 0.7 ]$width <- 6
visIgraph(tmp_g)
??visIgraph
## global variables:
# only if necessary to limit running time
#| echo: true
#| warning: false
# sets the directory of location of this script as the current directory
# setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# load packages
require(pacman)
p_load('DT', 'associatoR', 'tidyverse', 'wordcloud', 'readxl', 'igraph', 'visNetwork')
# load data
setwd("../generated data")
dir()
# for first order associations example
associationData <- read_excel(path = "word_associations_firstSecondOrder.xlsx", sheet = 1)
dim(associationData)
length(unique(associationData$participant_id))
table(associationData$participant_id)
sort(table(associationData$response), decreasing = TRUE)[1:10]
# Function to create edges from the grouped responses
create_edges <- function(association, distance = 2) {
edges <- data.frame()
n <- length(association)
for (i in 1:n) {
for (d in 1:distance) {
if (i + d <= n) {
edges <- rbind(edges, data.frame(from = association[i], to = association[i + d]))
}
}
}
return(edges)
}
generate_association_network <- function(cue_word, data_set, distance = 2) {
# Filter the dataset based on the cue word
tmp <- data_set[data_set$cue == cue_word, ]
# print number of times a response was given:
print("number of times a response was given")
print(sort(table(tmp$response)))
grouped_responses <- tmp %>%
group_by(participant_id) %>%
summarise(associations = list(response)) %>%
ungroup()
# Apply the function to create edges for each participant
all_edges <- do.call(rbind, lapply(grouped_responses$associations, create_edges, distance = distance))
# Calculate word frequencies across the entire data set
word_frequencies <- tmp %>%
count(response, name = "frequency")
# Calculate the edge weights (how often each link appears between two words)
edge_weights <- all_edges %>%
group_by(from, to) %>%
summarise(weight = n(), .groups = "drop")
# Create the graph from the edge list with edge weights
g <- graph_from_data_frame(edge_weights, directed = FALSE)
# Get the list of unique words in the graph (vertices)
vertices <- V(g)$name
# Extract edge weights to adjust the edge widths
edge_widths <- E(g)$weight
# Map word frequencies to vertex sizes
vertex_sizes <- sapply(vertices, function(word) {
freq <- word_frequencies %>%
filter(response == word) %>%
pull(frequency)
if(length(freq) == 0) {
return(1)  # Default size if the word is not found (shouldn't happen)
} else {
return(freq)
}
})
# Plot the network with vertex sizes proportional to word frequencies and edge widths proportional to edge weights
plot(g,
vertex.label.color = "black",
vertex.size = vertex_sizes * 0.5,           # Scale vertex sizes by word frequency
edge.width = edge_widths * 1,             # Scale edge widths by weight (number of links)
edge.arrow.size = 0.5)
# Return the igraph object
return(g)
}
tmp_g <- generate_association_network("underweight", associationData, distance = 2)
# interactive network:
E(tmp_g)$width <- 1
E(tmp_g)$weight <- E(tmp_g)$weight  / max(E(tmp_g)$weight )
E(tmp_g)[ weight > 0.3 ]$width <- 2
E(tmp_g)[ weight > 0.4 ]$width <- 3
E(tmp_g)[ weight > 0.5 ]$width <- 4
E(tmp_g)[ weight > 0.6 ]$width <- 5
E(tmp_g)[ weight > 0.7 ]$width <- 6
visIgraph(tmp_g)
tmp_g <- generate_association_network("fragile", associationData, distance = 2)
# Importing the associationData data
ar_obj <- ar_import(data = associationData,
participant = participant_id,
cue = cue,
response = response,
participant_vars = c(gender),
response_vars = c(response_position, response_level))
# normalize responses
ar_obj <- ar_normalize(ar_obj,
case = "most_frequent",
punct = "all",
whitespace = "squish",
process_cues = TRUE)
# set targets
ar_obj <- ar_set_targets(ar_obj, targets = "cues")
# embed associations
ar_obj <- ar_embed_targets(associations = ar_obj, method = "ppmi-svd")  %>%
ar_cluster_targets(method = "louvain",
similarity = "cosine",
resolution = 1.1)
# characterize targets using additional resources
ar_obj <- ar_characterize_targets(
ar_obj,
characteristics = c("valence", "arousal", "dominance"),
case_sensitive = FALSE
) # "word_frequency", "concreteness"
# correlate targets responses with participant variables
ar_obj <- ar_correlate_targets(
ar_obj,
participant_vars = c(gender),
metric = "auto"
)
df_targets <- as.data.frame(ar_obj$targets)
df_targets[order(df_targets$valence),]
ar_obj$targets$target[order(ar_obj$targets$gender_corr)[1:5]] # more frequently by women
ar_obj$targets$target[order(ar_obj$targets$gender_corr, decreasing = TRUE)[1:5]] # more frequently by man
sum(associationData$response == "model")
table(associationData$gender[associationData$response == "model"])
ar_obj %>%
plot(facet_by = "gender", top_n = 10)
table(ar_obj$targets$cluster)
ar_plot_embedding(
ar_obj,
color_by = cluster,
color_set = "G",
alpha = 0.5,
proportion_labels = 1
)
ar_plot_wordcloud(
ar_obj,
facet_col = cluster,
facet_row = gender,
color_by = cluster,
top_n = 20
)
tmp <- ar_cross_targets(ar_obj,
participant_vars = c(gender),
target_var = cluster,
normalize = TRUE)
DT::datatable(data = tmp)
ar_compare_targets(ar_obj,
participant_vars = c(gender),
target_var = valence,
fun = median, na.rm = TRUE)
sort(table(associationData$response), decreasing = TRUE)[1:10]
names(sort(table(associationData$response), decreasing = TRUE)[1:10])
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
for(t in top10){
associationData$gender[associationData$response == t]
}
associationData$gender[associationData$response == t]
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / sum(count))
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
gender_distribution
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(count))
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / sum(count))
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData) / 2)
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:10])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top10 <- names(sort(table(associationData$response), decreasing = TRUE)[1:20])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top10) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top12 <- names(sort(table(associationData$response), decreasing = TRUE)[1:12])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top12) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the plot
ggplot(gender_distribution, aes(x = response, y = proportion, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Response",
y = "Proportion",
fill = "Gender") +
theme_minimal()
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
# Create the rotated plot with increased space between bars
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity", position = position_dodge(width = 1.2)) +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
# Create the rotated plot with increased space between bars
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity", position = position_dodge(width = 1.5)) +
labs(title = "Proportion of Male and Female Responses for Top 10 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top12 <- names(sort(table(associationData$response), decreasing = TRUE)[1:12])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top12) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the rotated plot with increased space between bars
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity", position = position_dodge(width = 1.2)) +
labs(title = "Proportion of Male and Female Responses for Top 12 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top12 <- names(sort(table(associationData$response), decreasing = TRUE)[1:12])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top12) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the rotated plot with increased space between bars
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity") +
labs(title = "Proportion of Male and Female Responses for Top 12 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top12 <- names(sort(table(associationData$response), decreasing = TRUE)[1:12])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top12) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the rotated plot with increased space between bars
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity", position = position_dodge(width = 1)) +
labs(title = "Proportion of Male and Female Responses for Top 12 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
library(ggplot2)
library(dplyr)
# Get the top 10 most common responses
top12 <- names(sort(table(associationData$response), decreasing = TRUE)[1:12])
# Filter the data for the top 10 responses and count gender distribution
gender_distribution <- associationData %>%
filter(response %in% top12) %>%
group_by(response, gender) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(proportion = count / nrow(associationData))
# Create the rotated plot with increased space between bars
ggplot(gender_distribution, aes(x = proportion, y = response, fill = gender)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.5) +
labs(title = "Proportion of Male and Female Responses for Top 12 Responses",
x = "Proportion",
y = "Response",
fill = "Gender") +
theme_minimal()
ar_compare_targets(ar_obj,
participant_vars = c(gender),
target_var = valence,
fun = median, na.rm = TRUE)
