{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7102c40",
   "metadata": {},
   "source": [
    "# Different ways to call the Llama models offline\n",
    "\n",
    "**Information**\n",
    "\n",
    "To call the `meta-llama/Llama-3.1-8B-Instruct` model from Hugging Face offline, you can use several different methods depending on your preferences and technical requirements. \n",
    "\n",
    "Here are the most common approaches:\n",
    "\n",
    "1. Using the Hugging Face Transformers Library\n",
    "\n",
    "\n",
    "\n",
    "Remark: Llama models are published under the **META LLAMA 3 COMMUNITY LICENSE AGREEMENT**. The Meta Llama 3 Community License grants users a non-exclusive, royalty-free license(you not need to pay ongoing fees) to use, modify, and distribute Llama 3 materials, with requirements for attribution and naming conventions when creating derivative works. Users with over 700 million monthly active users need a separate license, and Meta disclaims all warranties and limits liability for any use of the materials.\n",
    "\n",
    "\n",
    "*** \n",
    "**Background information**\n",
    "\n",
    "* ...\n",
    "\n",
    "\n",
    "***\n",
    "**Coding sources**\n",
    "\n",
    "* You can run the `meta-llama/Llama-3.1-8B-Instruct`, see model page: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "* You could also run the `meta-llama/Meta-Llama-3-70B-Instruct`, see model page: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    + Hugging Face documentation: https://huggingface.co/docs/transformers/main/en/model_doc/llama3\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "Exemplify different approaches to call Llama (LLMs) offline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448c24c",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "## Define model\n",
    "\n",
    "It is highly recommend to use a small LLM, because using some approaches the LLMs is downloaded and stored on your computer (the model weights). It is highly likely that you do not have sufficient CPU and RAM and storage or your disk to run the larger (70B) model locally! See Llama 3.1 requirements: https://llamaimodel.com/requirements/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b13382",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # meta-llama/Meta-Llama-3-70B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eb47a",
   "metadata": {},
   "source": [
    "## Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded within the single code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3711c0b",
   "metadata": {},
   "source": [
    "## Get API key(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f150b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d3cf",
   "metadata": {},
   "source": [
    "Create simple prompts, which is identical for all of the following approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03204fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant specialized on animal names.\"\n",
    "user_content = \"\"\"\n",
    " Please write down five animals, provide only the names seperated by comma (\\,).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ea788",
   "metadata": {},
   "source": [
    "# Local approaches \n",
    "\n",
    "## Using the Hugging Face Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d72065e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ERROR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ERROR\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ERROR' is not defined"
     ]
    }
   ],
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2dfabe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'disk_offload'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer with disk offload\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, \n\u001b[0;32m      8\u001b[0m                                              device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      9\u001b[0m                                              torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, \n\u001b[0;32m     10\u001b[0m                                              disk_offload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Enable disk offload\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m     13\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\modeling_utils.py:3886\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3880\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   3881\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   3882\u001b[0m )\n\u001b[0;32m   3884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   3885\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 3886\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3888\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   3889\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[1;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'disk_offload'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load model and tokenizer with disk offload\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             disk_offload=True)  # Enable disk offload\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Once upon a time,\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc74859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882f580782a84b0e9c2438d3b06034c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\modeling_utils.py:4091\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4088\u001b[0m         device_map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[1;32m-> 4091\u001b[0m         dispatch_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4094\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mpostprocess_model(model)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\accelerate\\big_modeling.py:496\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    494\u001b[0m         model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m         )\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# Convert OrderedDict back to dict for easier usage\u001b[39;00m\n\u001b[0;32m    500\u001b[0m model\u001b[38;5;241m.\u001b[39mhf_device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(device_map)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Once upon a time,\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d808a59",
   "metadata": {},
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b18236f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  # is changed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Try a different model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fc5191",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cached_file' from 'transformers' (c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_file\n\u001b[0;32m      2\u001b[0m cached_file\u001b[38;5;241m.\u001b[39mclear_cache()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cached_file' from 'transformers' (c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import cached_file\n",
    "cached_file.clear_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa723e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fenn\\.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33402f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532dfcdd7e944b80b8446b545133cc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d46737dc0246a186a2b5762f0fd3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0414c4cba33e48918b28a040e02908b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166f9b529d1b41c49d0da5674c5b00d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer\n\u001b[1;32m----> 3\u001b[0m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, token\u001b[38;5;241m=\u001b[39mkey\u001b[38;5;241m.\u001b[39mhugging_api_key_pro2)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;66;03m# Prepare tokenizer initialization kwargs\u001b[39;00m\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;66;03m# Did we saved some inputs and kwargs to reload ?\u001b[39;00m\n\u001b[0;32m   2270\u001b[0m tokenizer_config_file \u001b[38;5;241m=\u001b[39m resolved_vocab_files\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_config_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokenizer_config_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tokenizer_config_handle:\n\u001b[0;32m   2273\u001b[0m         init_kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(tokenizer_config_handle)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2503\u001b[0m         obj\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m-> 2505\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   2506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_added_tokens(o, save\u001b[38;5;241m=\u001b[39msave, add_type_field\u001b[38;5;241m=\u001b[39madd_type_field) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py:171\u001b[0m, in \u001b[0;36mLlamaTokenizer.__init__\u001b[1;34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_default_system_prompt \u001b[38;5;241m=\u001b[39m use_default_system_prompt\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spm_processor(kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;241m=\u001b[39m add_prefix_space\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    175\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    176\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    188\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py:198\u001b[0m, in \u001b[0;36mLlamaTokenizer.get_spm_processor\u001b[1;34m(self, from_slow)\u001b[0m\n\u001b[0;32m    196\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model_kwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy \u001b[38;5;129;01mor\u001b[39;00m from_slow:  \u001b[38;5;66;03m# no dependency on protobuf\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mLoad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromFile(model_file)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceProcessor_LoadFromFile(\u001b[38;5;28mself\u001b[39m, arg)\n",
      "\u001b[1;31mTypeError\u001b[0m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "LlamaTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", force_download=True, token=key.hugging_api_key_pro2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70995684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39mkey\u001b[38;5;241m.\u001b[39mhugging_api_key_pro2)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading tokenizer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;66;03m# Prepare tokenizer initialization kwargs\u001b[39;00m\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;66;03m# Did we saved some inputs and kwargs to reload ?\u001b[39;00m\n\u001b[0;32m   2270\u001b[0m tokenizer_config_file \u001b[38;5;241m=\u001b[39m resolved_vocab_files\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_config_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokenizer_config_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tokenizer_config_handle:\n\u001b[0;32m   2273\u001b[0m         init_kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(tokenizer_config_handle)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2503\u001b[0m         obj\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m-> 2505\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   2506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_added_tokens(o, save\u001b[38;5;241m=\u001b[39msave, add_type_field\u001b[38;5;241m=\u001b[39madd_type_field) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py:171\u001b[0m, in \u001b[0;36mLlamaTokenizer.__init__\u001b[1;34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_default_system_prompt \u001b[38;5;241m=\u001b[39m use_default_system_prompt\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spm_processor(kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;241m=\u001b[39m add_prefix_space\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    175\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    176\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    188\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py:198\u001b[0m, in \u001b[0;36mLlamaTokenizer.get_spm_processor\u001b[1;34m(self, from_slow)\u001b[0m\n\u001b[0;32m    196\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model_kwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy \u001b[38;5;129;01mor\u001b[39;00m from_slow:  \u001b[38;5;66;03m# no dependency on protobuf\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mLoad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromFile(model_file)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceProcessor_LoadFromFile(\u001b[38;5;28mself\u001b[39m, arg)\n",
      "\u001b[1;31mTypeError\u001b[0m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "try:\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=key.hugging_api_key_pro2)\n",
    "except OSError as e:\n",
    "    print(\"Error loading tokenizer:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e945016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5ba088096b4db2b754c2806531c121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m      7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name, token\u001b[38;5;241m=\u001b[39mkey\u001b[38;5;241m.\u001b[39mhugging_api_key_pro)\n\u001b[1;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe(messages))\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\base.py:1268\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         )\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\base.py:1275\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1274\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1275\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1276\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\generation\\utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2045\u001b[0m     )\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2049\u001b[0m         input_ids,\n\u001b[0;32m   2050\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2051\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2052\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2053\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2054\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2055\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2056\u001b[0m     )\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2068\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\generation\\utils.py:3008\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3005\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3007\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[0;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1190\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1191\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1192\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1193\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1194\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1195\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1196\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1197\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1198\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1199\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1200\u001b[0m )\n\u001b[0;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1000\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    988\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    989\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    990\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m         position_embeddings,\n\u001b[0;32m    998\u001b[0m     )\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1000\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m   1001\u001b[0m         hidden_states,\n\u001b[0;32m   1002\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   1003\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1004\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1005\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1006\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1007\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1008\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m   1009\u001b[0m     )\n\u001b[0;32m   1011\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:745\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    744\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    746\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:311\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    309\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Who are you?\"}]\n",
    "pipe = pipeline(\"text-generation\", model=model_name, token=key.hugging_api_key_pro)\n",
    "response = pipe(\"Who are you?\", max_length=50)\n",
    "print(response[0][\"generated_text\"])\n",
    "print(pipe(messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e813d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\ncannot import name 'is_torchao_available' from 'transformers.utils' (c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[0;32m   1560\u001b[0m     [\n\u001b[0;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[0;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[0;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[0;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[0;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[0;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[0;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[0;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[0;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[0;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[0;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[0;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[0;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[0;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[0;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[0;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[0;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[0;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[0;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[0;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[0;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[0;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[0;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[0;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[0;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[0;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[0;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[0;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[0;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[0;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[0;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[0;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[0;32m   1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[0;32m   1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[0;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[0;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[0;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[0;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[0;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[0;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[0;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[0;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[1;32m-> 1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[0;32m   1604\u001b[0m     ]\n\u001b[0;32m   1605\u001b[0m )\n\u001b[0;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:40\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     35\u001b[0m     CausalLMOutputWithCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, SequenceSummary\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\modeling_utils.py:58\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     Conv1D,\n\u001b[0;32m     42\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     prune_linear_layer,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     DUMMY_INPUTS,\n\u001b[0;32m     50\u001b[0m     FLAX_WEIGHTS_NAME,\n\u001b[0;32m     51\u001b[0m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[0;32m     52\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[0;32m     53\u001b[0m     TF2_WEIGHTS_NAME,\n\u001b[0;32m     54\u001b[0m     TF_WEIGHTS_NAME,\n\u001b[0;32m     55\u001b[0m     WEIGHTS_INDEX_NAME,\n\u001b[0;32m     56\u001b[0m     WEIGHTS_NAME,\n\u001b[0;32m     57\u001b[0m     ContextManagers,\n\u001b[1;32m---> 58\u001b[0m     ModelOutput,\n\u001b[0;32m     59\u001b[0m     PushToHubMixin,\n\u001b[0;32m     60\u001b[0m     cached_file,\n\u001b[0;32m     61\u001b[0m     copy_func,\n\u001b[0;32m     62\u001b[0m     download_url,\n\u001b[0;32m     63\u001b[0m     has_file,\n\u001b[0;32m     64\u001b[0m     is_accelerate_available,\n\u001b[0;32m     65\u001b[0m     is_bitsandbytes_available,\n\u001b[0;32m     66\u001b[0m     is_offline_mode,\n\u001b[0;32m     67\u001b[0m     is_remote_url,\n\u001b[0;32m     68\u001b[0m     is_safetensors_available,\n\u001b[0;32m     69\u001b[0m     is_torch_tpu_available,\n\u001b[0;32m     70\u001b[0m     logging,\n\u001b[0;32m     71\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\quantizers\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, AutoQuantizationConfig\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfQuantizer\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\quantizers\\auto.py:18\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     AqlmConfig,\n\u001b[0;32m     20\u001b[0m     AwqConfig,\n\u001b[0;32m     21\u001b[0m     BitsAndBytesConfig,\n\u001b[0;32m     22\u001b[0m     CompressedTensorsConfig,\n\u001b[0;32m     23\u001b[0m     EetqConfig,\n\u001b[0;32m     24\u001b[0m     FbgemmFp8Config,\n\u001b[0;32m     25\u001b[0m     GPTQConfig,\n\u001b[0;32m     26\u001b[0m     HqqConfig,\n\u001b[0;32m     27\u001b[0m     QuantizationConfigMixin,\n\u001b[0;32m     28\u001b[0m     QuantizationMethod,\n\u001b[0;32m     29\u001b[0m     QuantoConfig,\n\u001b[0;32m     30\u001b[0m     TorchAoConfig,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer_aqlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AqlmHfQuantizer\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_auto_awq_available, is_hqq_available, is_torch_available, is_torchao_available, logging\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_torchao_available' from 'transformers.utils' (c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Simple test to ensure the pipeline works\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm a model,\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:779\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    778\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 779\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    780\u001b[0m     model,\n\u001b[0;32m    781\u001b[0m     model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    782\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    783\u001b[0m     framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    784\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    787\u001b[0m )\n\u001b[0;32m    789\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    790\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\pipelines\\base.py:234\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer_framework_load_model\u001b[39m(\n\u001b[0;32m    206\u001b[0m     model,\n\u001b[0;32m    207\u001b[0m     config: AutoConfig,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Select framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    If `model` is instantiated, this function will just infer the framework from the model class. Otherwise `model` is\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    actually a checkpoint name and this method will try to instantiate it using `model_classes`. Since we don't want to\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    instantiate the model twice, this model is returned for use by the pipeline.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    If both frameworks are installed and available for `model`, PyTorch is selected.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m        model (`str`, [`PreTrainedModel`] or [`TFPreTrainedModel]`):\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m            The model to infer the framework from. If `str`, a checkpoint name. The model to infer the framewrok from.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m        config ([`AutoConfig`]):\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m            The config associated with the model to help using the correct class\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        model_classes (dictionary `str` to `type`, *optional*):\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m            A mapping framework to class.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m        task (`str`):\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m            The task defining which pipeline will be returned.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m        model_kwargs:\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m            **model_kwargs)` function.\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m        `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1594\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1549\u001b[0m PEFT_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the peft library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124mpeft`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[0;32m   1560\u001b[0m     [\n\u001b[0;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[0;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[0;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[0;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[0;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[0;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[0;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[0;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[0;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[0;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[0;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[0;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[0;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[0;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[0;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[0;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[0;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[0;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[0;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[0;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[0;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[0;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[0;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[0;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[0;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[0;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[0;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[0;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[0;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[0;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[0;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[0;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[0;32m   1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[1;32m-> 1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[0;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[0;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[0;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[0;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[0;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[0;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[0;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[0;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[0;32m   1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[0;32m   1604\u001b[0m     ]\n\u001b[0;32m   1605\u001b[0m )\n\u001b[0;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n\u001b[0;32m   1609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(backends, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1549\u001b[0m PEFT_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the peft library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124mpeft`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[0;32m   1560\u001b[0m     [\n\u001b[0;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[0;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[0;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[0;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[0;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[0;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[0;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[0;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[0;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[0;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[0;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[0;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[0;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[0;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[0;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[0;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[0;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[0;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[0;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[0;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[0;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[0;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[0;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[0;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[0;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[0;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[0;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[0;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[0;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[0;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[0;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[0;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[1;32m-> 1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[0;32m   1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[0;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[0;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[0;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[0;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[0;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[0;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[0;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[0;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[0;32m   1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[0;32m   1604\u001b[0m     ]\n\u001b[0;32m   1605\u001b[0m )\n\u001b[0;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n\u001b[0;32m   1609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(backends, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1549\u001b[0m PEFT_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the peft library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124mpeft`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[0;32m   1560\u001b[0m     [\n\u001b[0;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[0;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[0;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[0;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[0;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[0;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[0;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[0;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[0;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[0;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[0;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[0;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[0;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[0;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[0;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[0;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[0;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[0;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[0;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[0;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[0;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[0;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[0;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[0;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[0;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[0;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[0;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[0;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[0;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[0;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[0;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[0;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[0;32m   1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[0;32m   1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[0;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[0;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[0;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[0;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[0;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[0;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[0;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[0;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[0;32m   1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[0;32m   1604\u001b[0m     ]\n\u001b[1;32m-> 1605\u001b[0m )\n\u001b[0;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n\u001b[0;32m   1609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(backends, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\ncannot import name 'is_torchao_available' from 'transformers.utils' (c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Simple test to ensure the pipeline works\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = pipe(\"Hello, I'm a model,\", max_length=50)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e07a331f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     ADAPTER_CONFIG_NAME,\n\u001b[0;32m      3\u001b[0m     ADAPTER_SAFE_WEIGHTS_NAME,\n\u001b[0;32m      4\u001b[0m     ADAPTER_WEIGHTS_NAME,\n\u001b[0;32m      5\u001b[0m     CONFIG_NAME,\n\u001b[0;32m      6\u001b[0m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[0;32m      7\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[0;32m      8\u001b[0m     WEIGHTS_INDEX_NAME,\n\u001b[0;32m      9\u001b[0m     WEIGHTS_NAME,\n\u001b[0;32m     10\u001b[0m     XLA_FSDPV2_MIN_VERSION,\n\u001b[0;32m     11\u001b[0m     PushInProgress,\n\u001b[0;32m     12\u001b[0m     PushToHubMixin,\n\u001b[0;32m     13\u001b[0m     can_return_loss,\n\u001b[0;32m     14\u001b[0m     find_labels,\n\u001b[0;32m     15\u001b[0m     is_accelerate_available,\n\u001b[0;32m     16\u001b[0m     is_apex_available,\n\u001b[0;32m     17\u001b[0m     is_bitsandbytes_available,\n\u001b[0;32m     18\u001b[0m     is_datasets_available,\n\u001b[0;32m     19\u001b[0m     is_galore_torch_available,\n\u001b[0;32m     20\u001b[0m     is_grokadamw_available,\n\u001b[0;32m     21\u001b[0m     is_in_notebook,\n\u001b[0;32m     22\u001b[0m     is_ipex_available,\n\u001b[0;32m     23\u001b[0m     is_liger_kernel_available,\n\u001b[0;32m     24\u001b[0m     is_lomo_available,\n\u001b[0;32m     25\u001b[0m     is_peft_available,\n\u001b[0;32m     26\u001b[0m     is_safetensors_available,\n\u001b[0;32m     27\u001b[0m     is_sagemaker_dp_enabled,\n\u001b[0;32m     28\u001b[0m     is_sagemaker_mp_enabled,\n\u001b[0;32m     29\u001b[0m     is_schedulefree_available,\n\u001b[0;32m     30\u001b[0m     is_torch_compile_available,\n\u001b[0;32m     31\u001b[0m     is_torch_mlu_available,\n\u001b[0;32m     32\u001b[0m     is_torch_mps_available,\n\u001b[0;32m     33\u001b[0m     is_torch_musa_available,\n\u001b[0;32m     34\u001b[0m     is_torch_neuroncore_available,\n\u001b[0;32m     35\u001b[0m     is_torch_npu_available,\n\u001b[0;32m     36\u001b[0m     is_torch_xla_available,\n\u001b[0;32m     37\u001b[0m     is_torch_xpu_available,\n\u001b[0;32m     38\u001b[0m     is_torchao_available,\n\u001b[0;32m     39\u001b[0m     logging,\n\u001b[0;32m     40\u001b[0m     strtobool,\n\u001b[0;32m     41\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .utils import (\n",
    "    ADAPTER_CONFIG_NAME,\n",
    "    ADAPTER_SAFE_WEIGHTS_NAME,\n",
    "    ADAPTER_WEIGHTS_NAME,\n",
    "    CONFIG_NAME,\n",
    "    SAFE_WEIGHTS_INDEX_NAME,\n",
    "    SAFE_WEIGHTS_NAME,\n",
    "    WEIGHTS_INDEX_NAME,\n",
    "    WEIGHTS_NAME,\n",
    "    XLA_FSDPV2_MIN_VERSION,\n",
    "    PushInProgress,\n",
    "    PushToHubMixin,\n",
    "    can_return_loss,\n",
    "    find_labels,\n",
    "    is_accelerate_available,\n",
    "    is_apex_available,\n",
    "    is_bitsandbytes_available,\n",
    "    is_datasets_available,\n",
    "    is_galore_torch_available,\n",
    "    is_grokadamw_available,\n",
    "    is_in_notebook,\n",
    "    is_ipex_available,\n",
    "    is_liger_kernel_available,\n",
    "    is_lomo_available,\n",
    "    is_peft_available,\n",
    "    is_safetensors_available,\n",
    "    is_sagemaker_dp_enabled,\n",
    "    is_sagemaker_mp_enabled,\n",
    "    is_schedulefree_available,\n",
    "    is_torch_compile_available,\n",
    "    is_torch_mlu_available,\n",
    "    is_torch_mps_available,\n",
    "    is_torch_musa_available,\n",
    "    is_torch_neuroncore_available,\n",
    "    is_torch_npu_available,\n",
    "    is_torch_xla_available,\n",
    "    is_torch_xpu_available,\n",
    "    is_torchao_available,\n",
    "    logging,\n",
    "    strtobool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "443af867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anglerfish, vampire squid, frilled shark, giant isopod, deep-sea jellyfish, colosal squid.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "# additional imports\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = InferenceClient(token=key.hugging_api_key_pro2)\n",
    "\n",
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant.\"\n",
    "user_content = \"\"\"\n",
    "   Please name five animals living in the deep sea. Only name animals seperated by commas.\n",
    "\"\"\"\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "   model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    max_tokens=500,\n",
    "        stream=False,\n",
    "        temperature=0.0\n",
    ")\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(text, 100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
