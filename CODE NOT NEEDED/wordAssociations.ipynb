{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate free associations\n",
    "\n",
    "**Information**\n",
    "\n",
    "The primary advantages of using free associations in psychological research include the ease of capturing spontaneous, unfiltered responses that offer insights into participants' natural thought processes. This method allows for exploring individual heterogeneity and minimizes researcher bias by reducing demand characteristics. It is scalable, making it suitable for large sample sizes, and reveals lay persons understandings of key constructs. The R package `associatoR` facilitates the analysis of free association data, offering tools for data processing, embedding, clustering, and visualization to explore mental representations using natural language processing techniques.\n",
    "\n",
    "he R package `associatoR` was developed by Samuel Aeschbach, see: https://github.com/samuelae/associatoR\n",
    "\n",
    "Remark: Work published under the **GNU General Public License Version 3** (GPLv3), published on June 29, 2007, allows users to freely use, modify, and distribute software.\n",
    "\n",
    "*** \n",
    "**Background information**\n",
    "\n",
    "* The idea of generating 5 responses for a single cue is, for example, implemented in the \"Word Association Study\": https://smallworldofwords.org/en/introduction\n",
    "    + and can be used to get, for example, the semantic representation of risk: Wulff, D. U., & Mata, R. (2022). On the semantic representation of risk. Science Advances, 8(27). https://doi.org/10.1126/sciadv.abm1883\n",
    "\n",
    "![wordAssociationStudy.png](attachment:wordAssociationStudy.png)\n",
    "\n",
    "* Possible extension: such associations could be enriched by emotional ratings, as done in the \"Affective Imagery Technique\"\n",
    "    + Leiserowitz, A. A. (2005). American Risk Perceptions: Is Climate Change Dangerous? Risk Analysis, 25(6), 1433–1442. https://doi.org/10.1111/j.1540-6261.2005.00690.x\n",
    "\n",
    "***\n",
    "**Coding sources**\n",
    "\n",
    "* To create associations the `meta-llama/Meta-Llama-3-70B-Instruct` is applied, see model page: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    + Hugging Face documentation: https://huggingface.co/docs/transformers/main/en/model_doc/llama3\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Aim of the code template**\n",
    "\n",
    "(a) Create word associations and (b) analyze the data using the developed R package (see \"analyze generated data\" folder). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word associations - single run\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from openai import OpenAI\n",
    "import huggingface_hub\n",
    "\n",
    "from collections import Counter\n",
    "from huggingface_hub import InferenceClient\n",
    "import pandas as pd\n",
    "\n",
    "import re # regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get API key(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('../..','src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt to create and store word associations\n",
    "\n",
    "Remark:\n",
    "It appears that large language models (LLMs) are inherently optimized to avoid redundancy, as they are often trained to treat repetition as undesirable in most natural language generation tasks. Consequently, even when prompted to allow repetition, the model may actively avoid repeating associations. However, in our use case, repetition is essential. To address this, the prompt has been explicitly designed to make repetition mandatory, and an example is provided to clarify the expected output. This ensures the model consistently includes repeated associations where relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_template: input_variables=['cue_word'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['cue_word'], template='\\nCue word: {cue_word}\\n'))]\n",
      "user_template_invoked: messages=[HumanMessage(content='\\nCue word: overweight\\n')]\n"
     ]
    }
   ],
   "source": [
    "system_template = \"\"\"\n",
    "<Context>\n",
    "You are an AI designed to generate associations to specific cues, reflecting the typical thoughts, ideas, and perceptions of five independent adults.\n",
    "Your task is to ensure that the associations are representative of the societal views and attitudes of these five persons.\n",
    "</Context>\n",
    "\n",
    "<Task>\n",
    "Generate five lists of associations based on the given cue word, whereby each list should contain five associations.\n",
    "Each list must represent associations to the cue word, and **it is important that important associations are repeated across different lists**.\n",
    "In particular,repeated association **must appear in at least two lists, and it can appear in all five lists** if it is highly relevant to the cue word.\n",
    "\n",
    "Here is an example format with some associations repeated across lists:\n",
    "\n",
    "association 1, association 2, association 3, association 4, association 5;  \n",
    "association 1, association 3, association 6, association 4, association 5;  \n",
    "association 2, association 4, association 7, association 3, association 1;  \n",
    "association 5, association 1, association 2, association 3, association 6;  \n",
    "association 3, association 1, association 5, association 7, association 4;\n",
    "\n",
    "Provide the output in the same format, with each list of associations separated by a semicolon (\\;):\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "user_template = \"\"\"\n",
    "Cue word: {cue_word}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", user_template)]\n",
    ")\n",
    "print(\"user_template:\", user_template)\n",
    "\n",
    "user_template_invoked = user_template.invoke({\"cue_word\": \"overweight\"})\n",
    "print(\"user_template_invoked:\", user_template_invoked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create and store word associations (single run)\n",
    "\n",
    "**Technical Considerations:**\n",
    "\n",
    "When querying the API, outputs are automatically cached if the inputs are identical. This applies to our case, as explained in the documentation: https://huggingface.co/docs/api-inference/parameters\n",
    "\n",
    "> To bypass caching and ensure fresh results for each query, it’s necessary to define the header: `\"x-use-cache\": \"false\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client (assuming the client and API key are already set up)\n",
    "client = InferenceClient(headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)\n",
    "\n",
    "\n",
    "# Feed prompts into model, define hyperparameters\n",
    "chat_completion = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_template},\n",
    "        {\"role\": \"user\", \"content\": user_template_invoked.messages[0].content}\n",
    "        ],\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    temperature=0.5,\n",
    "    stream=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: Obesity, health, diet, exercise, weight loss\n",
      "Person 2: Health risks, diet, exercise, weight loss, obesity\n",
      "Person 3: Weight gain, diet, exercise, obesity, self-esteem\n",
      "Person 4: Diet, exercise, weight loss, health, self-esteem\n",
      "Person 5: Health risks, weight gain, diet, exercise, obesity\n",
      "list_of_lists: [['Obesity', 'health', 'diet', 'exercise', 'weight loss'], ['Health risks', 'diet', 'exercise', 'weight loss', 'obesity'], ['Weight gain', 'diet', 'exercise', 'obesity', 'self-esteem'], ['Diet', 'exercise', 'weight loss', 'health', 'self-esteem'], ['Health risks', 'weight gain', 'diet', 'exercise', 'obesity']]\n"
     ]
    }
   ],
   "source": [
    "message_content = chat_completion.choices[0].message.content\n",
    "\n",
    "# Split the string after each occurrence of \";\"\n",
    "split_arrays = message_content.split(';')\n",
    "\n",
    "# remove whitespace from the split arrays\n",
    "split_arrays = [arr.strip() for arr in split_arrays]\n",
    "\n",
    "# Print the split arrays\n",
    "list_of_lists = []\n",
    "for i, arr in enumerate(split_arrays):\n",
    "    if i < 5:\n",
    "        print(f\"Person {i+1}: {arr}\")\n",
    "        list_of_lists.append(arr.strip().split(', '))\n",
    "    \n",
    "print(\"list_of_lists:\", list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "25\n",
      "Health risks\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_lists))\n",
    "print(len(list_of_lists) * 5)\n",
    "print(list_of_lists[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if some associations have been repeated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obesity': 4, 'health': 2, 'diet': 5, 'exercise': 5, 'weight loss': 3, 'health risks': 2, 'weight gain': 2, 'self-esteem': 2}\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists into a single list and convert each word to lowercase\n",
    "flattened_list = [word.lower() for sublist in list_of_lists for word in sublist]\n",
    "\n",
    "# Count occurrences of each word\n",
    "word_counts = Counter(flattened_list)\n",
    "\n",
    "# Filter words that appear more than once\n",
    "repeated_words = {word: count for word, count in word_counts.items() if count > 1}\n",
    "\n",
    "# Print the repeated words and their counts\n",
    "print(repeated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Association Generation - Process X Cues to Produce Y Responses\n",
    "\n",
    "The following code generates word associations by processing a specified number of cues (X, 4 in our case) and generating a corresponding set of output responses (Y). It can be enhanced with independent variables (IV) that account for various cognitive and demographic perspectives, such as:\n",
    "\n",
    "- Typical thoughts, ideas, and perceptions of a **child versus an adult**; resulting in 4*2*10 (25 responses each) = 80 API calls\n",
    "- Typical thoughts, ideas, and perceptions of a **female / male X adult / child**; resulting in 4*2*2*10 (25 responses each) = 160 API calls\n",
    "\n",
    "These variables allow for more personalized and context-aware word associations, reflecting differences in cognition across age, gender, and other demographic factors to test hypotheses regarding:\n",
    "\n",
    "**Gender differences**:\n",
    "*Possible hypothesis: There could be gender-based differences in cognitive associations due to societal roles, expectations, or differences in lived experiences. For example, males and females may have different associations with specific cues because of how they are socialized to perceive the world.*\n",
    "\n",
    "**Differences depending on own weight**:\n",
    "*Possible hypothesis: An individual's body weight could influence cognitive associations, particularly in contexts like body image, self-esteem, and food-related cues. People with higher or lower body weight might have distinct reactions to certain words due to personal experiences with societal attitudes toward weight.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust system prompt to create and store word associations\n",
    "\n",
    "Our aim is to introduce some dependency in the associations, because in our \"real-life\" study one person provides associations regarding four different cue words: \"overweight\", \"underweight\", \"normal weight\", \"muscular\".\n",
    "\n",
    "\n",
    "The prompt is in German, you could easily translate it to any language using DeepL or ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_template: input_variables=['cue_word1', 'cue_word2', 'cue_word3', 'cue_word4'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['cue_word1', 'cue_word2', 'cue_word3', 'cue_word4'], template='\\n1. Hinweiswort: {cue_word1}\\n2. Hinweiswort: {cue_word2}\\n3. Hinweiswort: {cue_word3}\\n4. Hinweiswort: {cue_word4}\\n'))]\n",
      "user_template_invoked: messages=[HumanMessage(content='\\n1. Hinweiswort: übergewichtig\\n2. Hinweiswort: untergewichtig\\n3. Hinweiswort: normalgewichtig\\n4. Hinweiswort: muskulös\\n')]\n"
     ]
    }
   ],
   "source": [
    "system_template = \"\"\"\n",
    "<Kontext>\n",
    "Du bist eine KI, die dafür entwickelt wurde, Assoziationen zu spezifischen Hinweisworten zu generieren, die die typischen Gedanken, Ideen und Wahrnehmungen von einem Erwachsenen widerspiegeln.\n",
    "Deine Aufgabe ist es sicherzustellen, dass die Assoziationen repräsentativ für die gesellschaftlichen Ansichten und Einstellungen dieser Person ist.\n",
    "</Kontext>\n",
    "\n",
    "<Aufgabe>\n",
    "Erstelle vier Listen von Assoziationen zu den gegebenen Hinweisworten, wobei jede Liste fünf Assoziationen enthalten soll.\n",
    "Jede Liste muss Assoziationen zum jeweiligen Hinweisworte darstellen.\n",
    "\n",
    "Hier ist ein Beispiel-Format mit einigen wiederholten Assoziationen über die Listen hinweg:\n",
    "\n",
    "1. Hinweiswort: [association 1, association 2, association 3, association 4, association 5];\n",
    "2. Hinweiswort: [association 1, association 2, association 3, association 4, association 5];\n",
    "3. Hinweiswort: [association 1, association 2, association 3, association 4, association 5];\n",
    "4. Hinweiswort: [association 1, association 2, association 3, association 4, association 5];\n",
    "\n",
    "Gib das Ergebnis im gleichen Format aus, wobei jede Liste von Assoziationen durch ein Semikolon (\\;) getrennt ist:\n",
    "</Aufgabe>\n",
    "\"\"\"\n",
    "\n",
    "user_template = \"\"\"\n",
    "1. Hinweiswort: {cue_word1}\n",
    "2. Hinweiswort: {cue_word2}\n",
    "3. Hinweiswort: {cue_word3}\n",
    "4. Hinweiswort: {cue_word4}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", user_template)]\n",
    ")\n",
    "print(\"user_template:\", user_template)\n",
    "\n",
    "user_template_invoked = user_template.invoke({\"cue_word1\": \"übergewichtig\", \"cue_word2\": \"untergewichtig\", \"cue_word3\": \"normalgewichtig\", \"cue_word4\": \"muskulös\"})\n",
    "print(\"user_template_invoked:\", user_template_invoked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create and store word associations (single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client (assuming the client and API key are already set up)\n",
    "client = InferenceClient(headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)\n",
    "\n",
    "\n",
    "# Feed prompts into model, define hyperparameters\n",
    "chat_completion = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_template},\n",
    "        {\"role\": \"user\", \"content\": user_template_invoked.messages[0].content}\n",
    "        ],\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    temperature=0.6,\n",
    "    stream=False,\n",
    "    max_tokens=1400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message_content: 1. Hinweiswort: übergewichtig; [Gesundheitsrisiken, Übergewicht, Fettleibigkeit, Diät, Ernährungsumstellung];\n",
      "2. Hinweiswort: untergewichtig; [Mangelernährung, Schwäche, Gesundheitsprobleme, Untergewicht, Ernährungsberatung];\n",
      "3. Hinweiswort: normalgewichtig; [Gesundheit, Wohlbefinden, Idealgewicht, Fitness, Ausgeglichenheit];\n",
      "4. Hinweiswort: muskulös; [Krafttraining, Fitnessstudio, Sport, Stärke, Attraktivität]; \n",
      "\n",
      "Cue 1: ['Gesundheitsrisiken', 'Übergewicht', 'Fettleibigkeit', 'Diät', 'Ernährungsumstellung']\n",
      "Cue 2: ['Mangelernährung', 'Schwäche', 'Gesundheitsprobleme', 'Untergewicht', 'Ernährungsberatung']\n",
      "Cue 3: ['Gesundheit', 'Wohlbefinden', 'Idealgewicht', 'Fitness', 'Ausgeglichenheit']\n",
      "Cue 4: ['Krafttraining', 'Fitnessstudio', 'Sport', 'Stärke', 'Attraktivität']\n",
      "list_of_lists: [['Gesundheitsrisiken', 'Übergewicht', 'Fettleibigkeit', 'Diät', 'Ernährungsumstellung'], ['Mangelernährung', 'Schwäche', 'Gesundheitsprobleme', 'Untergewicht', 'Ernährungsberatung'], ['Gesundheit', 'Wohlbefinden', 'Idealgewicht', 'Fitness', 'Ausgeglichenheit'], ['Krafttraining', 'Fitnessstudio', 'Sport', 'Stärke', 'Attraktivität']] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message_content = chat_completion.choices[0].message.content\n",
    "print(\"message_content:\", message_content, \"\\n\")\n",
    "\n",
    "# Regular expression to find content inside square brackets\n",
    "arrays = re.findall(r'\\[(.*?)\\]', message_content)\n",
    "# Split the arrays into lists of words\n",
    "split_arrays = [array.strip().split(', ') for array in arrays]\n",
    "\n",
    "list_of_lists = []\n",
    "# Display the extracted arrays\n",
    "for i, array in enumerate(split_arrays):\n",
    "    print(f\"Cue {i+1}: {array}\")\n",
    "    list_of_lists.append(array)\n",
    "\n",
    "print(\"list_of_lists:\", list_of_lists, \"\\n\")\n",
    "len(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create and store word associations (multiple runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed list iteration 1: [['Gesundheitsrisiken', 'Übergewicht', 'Diät', 'Fettleibigkeit', 'Ernährungsumstellung'], ['Mangelernährung', 'Schwäche', 'Gesundheitsprobleme', 'Untergewicht', 'Ernährungszusatz'], ['Gesundheit', 'Wohlbefinden', 'Idealgewicht', 'Fitness', 'Ausgeglichenheit'], ['Kraft', 'Fitnessstudio', 'Muskelaufbau', 'Athletik', 'Sportler']]\n",
      "First iteration\n",
      "Processed list iteration 2: [['Gesundheitsrisiken', 'Diät', 'Übergewicht', 'Fettleibigkeit', 'Selbstbewusstseinsprobleme'], ['Mangelernährung', 'Gesundheitsprobleme', 'Schwäche', 'Untergewicht', 'Erschöpfung'], ['Gesundheit', 'Fitness', 'Wohlbefinden', 'Idealgewicht', 'Selbstsicherheit'], ['Kraft', 'Fitness', 'Sport', 'Attraktivität', 'Selbstbewusstsein']]\n",
      "Not first iteration\n"
     ]
    }
   ],
   "source": [
    "# manually define vector of cue words\n",
    "vec_cues = [\"übergewichtig\", \"untergewichtig\", \"normalgewichtig\", \"muskulös\"]\n",
    "\n",
    "\n",
    "# Initialize the client (assuming the client and API key are already set up)\n",
    "client = InferenceClient(headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)\n",
    "\n",
    "\n",
    "for index, _ in enumerate(range(10)):\n",
    "    # Feed prompts into model, define hyperparameters\n",
    "    chat_completion = client.chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_template},\n",
    "            {\"role\": \"user\", \"content\": user_template_invoked.messages[0].content}\n",
    "            ],\n",
    "        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "        temperature=0.6,\n",
    "        stream=False,\n",
    "        max_tokens=1400\n",
    "    )\n",
    "    \n",
    "    \n",
    "    message_content = chat_completion.choices[0].message.content\n",
    "    \n",
    "    \n",
    "    # Regular expression to find content inside square brackets\n",
    "    arrays = re.findall(r'\\[(.*?)\\]', message_content)\n",
    "    # Split the arrays into lists of words\n",
    "    split_arrays = [array.strip().split(', ') for array in arrays]\n",
    "\n",
    "    list_of_lists = []\n",
    "    # Display the extracted arrays\n",
    "    for i, array in enumerate(split_arrays):\n",
    "        #print(f\"Cue {i+1}: {array}\")\n",
    "        list_of_lists.append(array)\n",
    "        \n",
    "    # Print or process the resulting list_of_lists\n",
    "    print(f\"Processed list iteration {index + 1}: {list_of_lists}\")\n",
    "    \n",
    "    # Save the list of lists to a file\n",
    "    # # > Transform into long format with an ID, independent variables, cue word, response and position\n",
    "    long_data = []\n",
    "    for idx, sublist in enumerate(list_of_lists, start=0):\n",
    "        for pos, item in enumerate(sublist, start=0):\n",
    "            long_data.append([\"Run_\" + str(index + 1) + \"_\" + str(idx + 1), \"none\", \"adult\", vec_cues[idx], item, pos + 1])\n",
    "            df_long_internal = pd.DataFrame(long_data, columns=[\"participant_id\", \"gender\", \"agegroup\", \"cue\", \"response\", \"response_position\"])\n",
    "            \n",
    "    # place the if block inside the loop\n",
    "    if index == 0:\n",
    "        print(\"First iteration\")\n",
    "        df_long = df_long_internal\n",
    "    else:\n",
    "        print(\"Not first iteration\")\n",
    "        df_long = pd.concat([df_long, df_long_internal])\n",
    "\n",
    "\n",
    "# save the resulting data frame to a file\n",
    "len(df_long)\n",
    "df_long.to_excel(\"generated data/word_associations.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client (assuming the client and API key are already set up)\n",
    "client = InferenceClient(headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)\n",
    "\n",
    "# List of cue words\n",
    "cue_words = [\"overweight\", \"underweight\", \"normal weight\", \"muscular\"]  # X\n",
    "\n",
    "# Loop through each cue word\n",
    "for cue_word in cue_words:\n",
    "    # Loop to call the client X times for each cue word, using enumerate to get the index\n",
    "    for index, _ in enumerate(range(10)):\n",
    "        # Invoke the user template with the current cue word\n",
    "        user_template_invoked = user_template.invoke({\"cue_word\": cue_word})\n",
    "        print(f\"Invoking user template for cue_word: {cue_word}, iteration: {index + 1}\")\n",
    "\n",
    "        # Feed prompts into model, define hyperparameters\n",
    "        chat_completion = client.chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_template},\n",
    "                {\"role\": \"user\", \"content\": user_template_invoked.messages[0].content},\n",
    "            ],\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "            temperature=0.3,\n",
    "            stream=False,\n",
    "        ) # Y\n",
    "\n",
    "        # get clean data set:\n",
    "        # > Get message content from the response\n",
    "        message_content = chat_completion.choices[0].message.content\n",
    "        # > Split the string after each occurrence of \";\"\n",
    "        split_arrays = message_content.split(\";\")\n",
    "        # > Remove whitespace from the split arrays\n",
    "        split_arrays = [arr.strip() for arr in split_arrays]\n",
    "        # > Create a list of lists from the split arrays\n",
    "        list_of_lists = []\n",
    "        for i, arr in enumerate(split_arrays):\n",
    "            list_of_lists.append(arr.strip().split(\", \"))\n",
    "\n",
    "        # Print or process the resulting list_of_lists\n",
    "        print(f\"Processed list for cue_word {cue_word}, iteration {index + 1}: {list_of_lists}\")\n",
    "\n",
    "        # Save the list of lists to a file\n",
    "        # > Transform into long format with an ID, independent variables, cue word, response and position\n",
    "        long_data = []\n",
    "        for idx, sublist in enumerate(list_of_lists, start=0):\n",
    "            for pos, item in enumerate(sublist, start=0):\n",
    "                long_data.append([cue_word + \"_\" + str(index + 1) + \"_\" + str(idx + 1), \"none\", \"adult\", cue_word, item, pos + 1])\n",
    "\n",
    "        df_long_internal = pd.DataFrame(long_data, columns=[\"participant_id\", \"gender\", \"agegroup\", \"cue\", \"response\", \"response_position\"])\n",
    "\n",
    "        # place the if block inside the loop\n",
    "        if index == 0 and cue_word == \"overweight\":\n",
    "            print(\"First iteration\")\n",
    "            df_long = df_long_internal\n",
    "        else:\n",
    "            print(\"Not first iteration\")\n",
    "            df_long = pd.concat([df_long, df_long_internal])\n",
    "\n",
    "\n",
    "# save the resulting data frame to a file\n",
    "len(df_long)\n",
    "df_long.to_excel(\"generated data/word_associations.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking user template for cue_word: overweight, iteration: 1\n",
      "Processed list for cue_word overweight, iteration 1: [['obesity', 'health risks', 'weight loss', 'dieting', 'exercise'], ['unhealthy eating', 'fast food', 'junk food', 'sedentary lifestyle', 'laziness'], ['self-esteem issues', 'body image', 'low confidence', 'mental health', 'anxiety'], ['medical conditions', 'diabetes', 'heart disease', 'high blood pressure', 'medication'], ['weight gain', 'metabolism', 'genetics', 'portion control', 'nutrition']]\n",
      "Invoking user template for cue_word: overweight, iteration: 2\n",
      "Processed list for cue_word overweight, iteration 2: [['obesity', 'health risks', 'weight loss', 'dieting', 'exercise'], ['unhealthy eating', 'fast food', 'junk food', 'sedentary lifestyle', 'laziness'], ['self-esteem issues', 'body image', 'low confidence', 'mental health', 'anxiety'], ['medical conditions', 'diabetes', 'heart disease', 'high blood pressure', 'medication'], ['weight gain', 'metabolism', 'genetics', 'portion control', 'nutrition']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the client (assuming the client and API key are already set up)\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "    api_key=key.hugging_api_key,\n",
    ")\n",
    "\n",
    "# List of cue words\n",
    "cue_words = [\"overweight\"] # , \"underweight\", \"normal weight\"\n",
    "\n",
    "# Loop through each cue word\n",
    "for cue_word in cue_words:\n",
    "    # Loop to call the client 10 times for each cue word, using enumerate to get the index\n",
    "    for index, _ in enumerate(range(2)):\n",
    "        # Invoke the user template with the current cue word\n",
    "        user_template_invoked = user_template.invoke({\"cue_word\": cue_word})\n",
    "        print(f\"Invoking user template for cue_word: {cue_word}, iteration: {index + 1}\")\n",
    "\n",
    "        #time.sleep(2.5)  # Pause for 0.5 seconds (500 milliseconds)\n",
    "\n",
    "        # Make the API call to chat completion\n",
    "        kwargs = dict(max_tokens=1024, temperature=.5, top_p=0.9)\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            **kwargs,\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_template},\n",
    "                {\"role\": \"user\", \"content\": user_template_invoked.messages[0].content},\n",
    "            ],\n",
    "            stream=False,\n",
    "            #temperature=1.1,\n",
    "            #top_p=0.9,  # Use nucleus sampling for more diversity\n",
    "            #frequency_penalty=1.0,  # Prevents the model from repeating the same response\n",
    "        )\n",
    "\n",
    "        # Get message content from the response\n",
    "        message_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        # Split the string after each occurrence of \";\"\n",
    "        split_arrays = message_content.split(';')\n",
    "\n",
    "        # Remove whitespace from the split arrays\n",
    "        split_arrays = [arr.strip() for arr in split_arrays]\n",
    "\n",
    "        # Create a list of lists from the split arrays\n",
    "        list_of_lists = []\n",
    "        for i, arr in enumerate(split_arrays):\n",
    "            list_of_lists.append(arr.strip().split(', '))\n",
    "\n",
    "        # Print or process the resulting list_of_lists\n",
    "        print(f\"Processed list for cue_word {cue_word}, iteration {index + 1}: {list_of_lists}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed list for cue_word overweight, iteration 1: [['obesity', 'health risks', 'diet', 'exercise', 'weight loss'], ['unhealthy eating', 'fast food', 'junk food', 'sedentary lifestyle', 'calories'], ['self-esteem', 'body image', 'low confidence', 'mental health', 'anxiety'], ['heart disease', 'diabetes', 'high blood pressure', 'stroke', 'medical conditions'], ['weight management', 'fitness goals', 'nutrition', 'wellness', 'healthy habits']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five animals living in Europe: Wolf, Bear, Fox, Rabbit, Deer.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import textwrap\n",
    "\n",
    "# Initialize client\n",
    "client = InferenceClient(headers={\"X-use-cache\": \"false\"}, token=key.hugging_api_key)\n",
    "#client = InferenceClient(token=key.hugging_api_key)\n",
    "\n",
    "# Create prompts\n",
    "system_content = \"You are a helpful assistant.\"\n",
    "user_content = \"\"\"\n",
    "   Please name five animals living in Europe. Only name animals seperated by commas.\n",
    "\"\"\"\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ],\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    temperature=.7,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(text, 100)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
